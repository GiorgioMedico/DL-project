{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ElNaMbLnRdHR"
      },
      "source": [
        "# Sentence Reconstruction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXr4iGUGRms8"
      },
      "source": [
        "The purpose of this project is to take in input a sequence of words corresponding to a random permutation of a given english sentence, and reconstruct the original sentence.\n",
        "\n",
        "The otuput can be either produced in a single shot, or through an iterative (autoregressive) loop generating a single token at a time.\n",
        "\n",
        "\n",
        "CONSTRAINTS:\n",
        "* No pretrained model can be used.\n",
        "* The neural network models should have less the 20M parameters.\n",
        "* No postprocessing should be done (e.g. no beamsearch)\n",
        "* You cannot use additional training data.\n",
        "\n",
        "\n",
        "BONUS PARAMETERS:\n",
        "\n",
        "A bonus of 0-2 points will be attributed to incentivate the adoption of models with a low number of parameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQ8k-L-WUK7l"
      },
      "source": [
        "# Dataset\n",
        "\n",
        "The dataset is composed by sentences taken from the generics_kb dataset of hugging face. We restricted the vocabolary to the 10K most frequent words, and only took sentences making use of this vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "nJ02vehGYySk",
        "outputId": "49ec9576-86cd-44e7-ea52-45d117028658"
      },
      "outputs": [],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "807Wk-ir_bDU"
      },
      "source": [
        "Download the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_WjtqA8TrHcS"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from keras.layers import TextVectorization\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "np.random.seed(42)\n",
        "ds = load_dataset('generics_kb',trust_remote_code=True)['train']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lAVLfsdc_ej5"
      },
      "source": [
        "Filter row with length greater than 8.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iznq8xGNt2Zr"
      },
      "outputs": [],
      "source": [
        "ds = ds.filter(lambda row: len(row[\"generic_sentence\"].split(\" \")) > 8 )\n",
        "corpus = [ '<start> ' + row['generic_sentence'].replace(\",\",\" <comma>\") + ' <end>' for row in ds ]\n",
        "corpus = np.array(corpus)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FyYpXLCF_ldR"
      },
      "source": [
        "Create a tokenizer and Detokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "T-bE2JpVbU9E"
      },
      "outputs": [],
      "source": [
        "tokenizer=TextVectorization( max_tokens=10000, standardize=\"lower_and_strip_punctuation\", encoding=\"utf-8\",) #con il max prende le piu frequenti. ordina i token del vocab dal piu frequente al meno frequente\n",
        "tokenizer.adapt(corpus)\n",
        "\n",
        "class TextDetokenizer:\n",
        "    def __init__(self, vectorize_layer):\n",
        "        self.vectorize_layer = vectorize_layer\n",
        "        vocab = self.vectorize_layer.get_vocabulary()\n",
        "        self.index_to_word = {index: word for index, word in enumerate(vocab)}\n",
        "\n",
        "    def __detokenize_tokens(self, tokens):\n",
        "        def check_token(t):\n",
        "          if t == 3:\n",
        "            s=\"<start>\"\n",
        "          elif t == 2:\n",
        "            s=\"<end>\"\n",
        "          elif t == 7:\n",
        "            s=\"<comma>\"\n",
        "          else:\n",
        "            s=self.index_to_word.get(t, '[UNK]')\n",
        "          return s\n",
        "\n",
        "        return ' '.join([ check_token(token) for token in tokens if token != 0])\n",
        "\n",
        "    def __call__(self, batch_tokens):\n",
        "       return [self.__detokenize_tokens(tokens) for tokens in batch_tokens]\n",
        "\n",
        "\n",
        "detokenizer = TextDetokenizer( tokenizer )\n",
        "sentences = tokenizer( corpus ).numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZ64sns1_pSK"
      },
      "source": [
        "Remove from corpus the sentences where any unknow word appears"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2LPQtryQz5wh"
      },
      "outputs": [],
      "source": [
        "mask = np.sum( (sentences==1), axis=1) >= 1\n",
        "original_data = np.delete( sentences, mask , axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qYfOscVk7U0r",
        "outputId": "4adeb4a1-efdf-4d8f-e4dd-2015e9818624"
      },
      "outputs": [],
      "source": [
        "original_data.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5puiiQ2D_uxa"
      },
      "source": [
        "Shuffle the sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ZXLkWB6od0R"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.utils import Sequence\n",
        "\n",
        "class DataGenerator(Sequence):\n",
        "    def __init__(self, data, batch_size=32, shuffle=True, seed=42):\n",
        "        self.data = data\n",
        "        self.batch_size = batch_size\n",
        "        self.shuffle = shuffle\n",
        "        self.seed = seed\n",
        "        self.on_epoch_end()\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.floor(len(self.data) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
        "\n",
        "        data_batch = np.array([self.data[k] for k in indexes])\n",
        "        #copy of ordered sequences\n",
        "        result = np.copy(data_batch)\n",
        "        #shuffle only the relevant positions for each batch\n",
        "        for i in range(data_batch.shape[0]):\n",
        "          np.random.shuffle(data_batch[i,1:data_batch[i].argmin() - 1])\n",
        "\n",
        "        return data_batch , result\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        self.indexes = np.arange(len(self.data))\n",
        "        if self.shuffle:\n",
        "            if self.seed is not None:\n",
        "                np.random.seed(self.seed)\n",
        "            np.random.shuffle(self.indexes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fNo_Jy3N8zHS"
      },
      "outputs": [],
      "source": [
        "# Make a random permutation of training and test set\n",
        "np.random.seed(42)\n",
        "# Shuffle the all data\n",
        "shuffled_indices = np.random.permutation(len(original_data))\n",
        "shuffled_data = original_data[shuffled_indices]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uNlq1Khx1oH2"
      },
      "outputs": [],
      "source": [
        "#split the dataset\n",
        "train_generator = DataGenerator(shuffled_data[:220000])\n",
        "test_generator = DataGenerator(shuffled_data[220000:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qR5xwMOn4E88",
        "outputId": "c15b3995-877e-493d-8f1d-2679bf0c01cc"
      },
      "outputs": [],
      "source": [
        "x, y = test_generator.__getitem__(1)\n",
        "x = detokenizer(x)\n",
        "y = detokenizer(y)\n",
        "\n",
        "for i in range(7):\n",
        "  print(\"original: \", y[i])\n",
        "  print(\"shuffled: \", x[i])\n",
        "  print(\"\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fo8MazCGBTv3"
      },
      "source": [
        "# Metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G0NOkuO0CfPo"
      },
      "source": [
        "Let s be the source string and p your prediction. The quality of the results will be measured according to the following metric:\n",
        "\n",
        "1.  look for the longest substring w between s and p\n",
        "2.  compute |w|/max(|s|,|p|)\n",
        "\n",
        "If the match is exact, the score is 1.\n",
        "\n",
        "When computing the score, you should NOT consider the start and end tokens.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-aUrdlXDdVf"
      },
      "source": [
        "The longest common substring can be computed with the SequenceMatcher function of difflib, that allows a simple definition of our metric."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ulpTRdrF_huh"
      },
      "outputs": [],
      "source": [
        "from difflib import SequenceMatcher\n",
        "\n",
        "def score(s,p):\n",
        "  match = SequenceMatcher(None, s, p).find_longest_match()\n",
        "  #print(match.size)\n",
        "  return (match.size/max(len(p),len(s)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RB2YfjXNExM-"
      },
      "source": [
        "Let's do an example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h17C8bVjEwur",
        "outputId": "4da07b5d-d8a9-41d7-bc92-f59a629e551b"
      },
      "outputs": [],
      "source": [
        "original = \"at first henry wanted to be friends with the king of france\"\n",
        "generated = \"henry wanted to be friends with king of france at the first\"\n",
        "\n",
        "print(\"your score is \",score(original,generated))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BET8GqBvFugR"
      },
      "source": [
        "The score must be computed as an average of at least 3K random examples taken form the test set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fwo7xj4GBW1"
      },
      "source": [
        "# What to deliver"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i6uITuxOGHfJ"
      },
      "source": [
        "You are supposed to deliver a single notebook, suitably commented.\n",
        "The notebook should describe a single model, although you may briefly discuss additional attempts you did.\n",
        "\n",
        "The notebook should contain a full trace of the training.\n",
        "Weights should be made available on request.\n",
        "\n",
        "You must also give a clear assesment of the performance of the model, computed with the metric that has been given to you.\n",
        "\n",
        "# Good work!"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
