{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AnSSftrAKasz"
      },
      "source": [
        "# Qlearning on the Grid World\n",
        "\n",
        "In this notebook we deploy the qlearning technique on a simple grid world problem.\n",
        "\n",
        "The grid world is composed by a set of states organized in a grid. \n",
        "The agent has only view of its current state, and can move to an adjacent state via four possible moves: left, up, right and down.\n",
        "\n",
        "One or more states are marked as winning states, and the goal is to teach the agent to reach them. \n",
        "\n",
        "After, a while, by trail and error, the agent learns the correct sequence of actions to complete the task.\n",
        "\n",
        "Learning is world-specific (and it could not be otherwise). \n",
        "if you change the world, you need to repeat training.\n",
        "\n",
        "The world grid is an abstraction: an encoding of a possible Markov Decision Process. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SjXBl3jaKWoh"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.cm as cm\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2oTWzKEANWbh"
      },
      "source": [
        "We start defining the dimensions of the grid, and the transitions corresponding to moves. Trying to pass the border of the grid results in a no-action."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xGB1uF28Nyw1"
      },
      "outputs": [],
      "source": [
        "Wdim = 4\n",
        "Hdim = 3\n",
        "\n",
        "def move(s,a):\n",
        "    i,j = s\n",
        "    if a==0: #left\n",
        "        i = max(i-1,0)\n",
        "    elif a==1: #up\n",
        "        j = min(j+1,Hdim-1)\n",
        "    elif a==2: #right\n",
        "        i = min(i+1,Wdim-1)\n",
        "    elif a==3: #down\n",
        "        j = max(j-1,0)\n",
        "    return (i,j)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZR6z666OYIK"
      },
      "source": [
        "Now we define the terminal states, and a function generating a random state, that will be used as the initial state for training episodes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C9q3SFDaOiDc"
      },
      "outputs": [],
      "source": [
        "def term(s):\n",
        "    return (s==(0,2) or s==(3,1))\n",
        "\n",
        "def random_state():\n",
        "    return (np.random.randint(Wdim),np.random.randint(Hdim))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cWaTD4CXVjvK"
      },
      "source": [
        "Now we define a couple of functions that will allow us to visualize the resulting Qfuntion and Vfunction in the form of heatmaps.\n",
        "\n",
        "Drawing Values is done in the obvious way. We normalize values in the range [0,1] and use the colormap facilities of pyplot."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vHoDYYJnVyuA"
      },
      "outputs": [],
      "source": [
        "def drawV(Vtable):\n",
        "    Vmax = np.max(Vtable)+.2\n",
        "    Vmin = np.min(Vtable)-.2\n",
        "    VtableNorm = (Vtable - Vmin)/(Vmax-Vmin)\n",
        "    xdim,ydim = Vtable.shape\n",
        "    print(xdim,ydim)\n",
        "    plt.figure()\n",
        "    plt.axis('off')\n",
        "\n",
        "    for i in range(xdim):\n",
        "      for j in range(ydim):\n",
        "          x = [i,i+1,i+1,i]\n",
        "          y = [j,j,j+1,j+1]\n",
        "          plt.text(i+.36,j+.46,\"{:.2f}\".format(Vtable[i,j]))\n",
        "          plt.fill(x,y,edgecolor='black',facecolor=cm.Reds(VtableNorm[i,j]))\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMi1WnXJXNsJ"
      },
      "source": [
        "The case of Qvalues is a bit nore problematic, since the Qtable has four dimensions. We split each cell in four along its diagonals, and put each qvalue q(s,a) in the cell corresponding to the action a, namely 0=left, 1=up, 2=right and 3=down."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H25AadwxX3a2"
      },
      "outputs": [],
      "source": [
        "def drawQ(Qtable,save_as=None):\n",
        "    Qmax = np.max(Qtable)+.2\n",
        "    Qmin = np.min(Qtable)-.2\n",
        "    QtableNorm = (Qtable - Qmin)/(Qmax-Qmin)\n",
        "    xdim,ydim,zdim = Qtable.shape\n",
        "    print(\"Qshape=\",xdim,ydim,zdim)\n",
        "    plt.figure()\n",
        "    plt.axis('off')\n",
        "\n",
        "    for i in range(xdim):\n",
        "      for j in range(ydim):\n",
        "          x = [i,i,i+.5]\n",
        "          y = [j,j+1,j+.5]\n",
        "          plt.text(i+.1,j+.46,\"{:.2f}\".format(Qtable[i,j,0]))\n",
        "          plt.fill(x,y,edgecolor='black',facecolor=cm.Reds(QtableNorm[i,j,0]))\n",
        "          x = [i,i+1,i+.5]\n",
        "          y = [j+1,j+1,j+.5]\n",
        "          plt.text(i+.35,j+.79,\"{:.2f}\".format(Qtable[i,j,1]))\n",
        "          plt.fill(x,y,edgecolor='black',facecolor=cm.Reds(QtableNorm[i,j,1]))\n",
        "          x = [i+1,i+1,i+.5]\n",
        "          y = [j+1,j,j+.5]\n",
        "          plt.text(i+.63,j+.46,\"{:.2f}\".format(Qtable[i,j,2]))\n",
        "          plt.fill(x,y,edgecolor='black',facecolor=cm.Reds(QtableNorm[i,j,2]))\n",
        "          x = [i,i+1,i+.5]\n",
        "          y = [j,j,j+.5]\n",
        "          plt.text(i+.35,j+.19,\"{:.2f}\".format(Qtable[i,j,3]))\n",
        "          plt.fill(x,y,edgecolor='black',facecolor=cm.Reds(QtableNorm[i,j,3]))\n",
        "    if save_as:\n",
        "        plt.savefig(save_as,bbox_inches='tight')\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUCsn26FPWLs"
      },
      "source": [
        "Let us come to the learning algorithm.\n",
        "\n",
        "As action reward, we just give a unitary negative reward for each step. In this way, the agent will learn to reach  a terminal state in a minimum number of steps."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dOtjh-D0QTLx"
      },
      "source": [
        "We can now initialize the  Qtable, the Vtable, and the main hyperparameters for the qlearning technique."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NFl_WNQlPtfG"
      },
      "outputs": [],
      "source": [
        "Qtable = np.zeros((Wdim,Hdim,4)) #4 actions\n",
        "Vtable = np.zeros((Wdim,Hdim))\n",
        "\n",
        "alpha = .01 #learning rate\n",
        "gamma = .95 #time discount\n",
        "epsilon = 1\n",
        "episodes = 10000\n",
        "exploitation_start = 5000\n",
        "final_randomicity = .1\n",
        "epsilon_rate = final_randomicity**(1./episodes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_R2kcuNGT2fN",
        "outputId": "e8672a9d-7d1a-45c9-8f94-3a0c95ef60c5"
      },
      "outputs": [],
      "source": [
        "verbose = True\n",
        "\n",
        "for n in range(0,episodes):\n",
        "    if verbose and (n % 1000 == 0):\n",
        "      print(\"episode no {}\".format(n))\n",
        "    s0 = random_state()\n",
        "    while not term(s0):\n",
        "        #choose action\n",
        "        if np.random.random() > epsilon:\n",
        "            a = np.argmax(Qtable[s0])\n",
        "        else:\n",
        "            a = np.random.randint(4)\n",
        "        s1 = move(s0,a)\n",
        "        T = term(s1)\n",
        "        if T:\n",
        "            R = -1\n",
        "        else:\n",
        "            R = -1 + gamma*np.max(Qtable[s1])\n",
        "        Qtable[s0][a] = Qtable[s0][a] + alpha*(R-Qtable[s0][a])\n",
        "        s0 = s1\n",
        "    if n > 5000: #no exploitation for the first 5000 iterations\n",
        "        epsilon = epsilon * epsilon_rate "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mv45lp9bYURZ"
      },
      "source": [
        "We can now draw the resulting optimal Qtable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "AZWQDwFtYSfh",
        "outputId": "66dea105-ae8c-496a-dd0f-a25471e32de4"
      },
      "outputs": [],
      "source": [
        "drawQ(Qtable)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "Ag88pC4BY4it",
        "outputId": "9da9f6ce-0844-41dc-d998-78720c5a5c58"
      },
      "outputs": [],
      "source": [
        "Vtable = np.max(Qtable,axis=-1)\n",
        "drawV(Vtable)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Grid_world.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
