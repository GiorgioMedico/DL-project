{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yW3Lmp7lWomZ"
      },
      "source": [
        "# Lab4: Object Detection vs Segmentation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WqeTtNgoWsRB"
      },
      "source": [
        "![ObjectDetection](https://drive.google.com/uc?id=1AeG8F2TYEMUWy6huTWBDPsU0NnLr-pyK ).\n",
        "\n",
        "\n",
        "Object detection detects targets with bounding boxes to enable their classification and localization in an image.\n",
        "\n",
        "\n",
        "The segmentation problem in computer vision refers to the division of an image into meaningful regions or segments based on features such as color, intensity, texture, or contours. The primary objective is to distinguish and identify the objects within the image by accurately delineating the boundaries between them.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JETU9aiJAtAk"
      },
      "source": [
        "![Image Segmentation](https://drive.google.com/uc?id=1TABoIVgRUlUAEaFe_jzG0aOiQM5dk9qX )\n",
        "\n",
        "From a computational standpoint, Segmentation can be interpreted as a pixel-level classification problem.\n",
        "\n",
        "Segmentation provides fine-grained information about object boundaries and regions, while detection focuses on identifying specific objects and their locations.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1dnfZRHU4w3"
      },
      "source": [
        "**Some uses cases**\n",
        "\n",
        "\n",
        "*   Autonomous Vehicles\n",
        "*   Medical Imaging Analysis\n",
        "*   Analysis of Satellite Images\n",
        "*   Smart Agriculture\n",
        "*   Industrial Inspection\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HhXgnGzH-aCM"
      },
      "source": [
        "###Intersection over union"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IbGZf-7y-pAs"
      },
      "source": [
        "The Intersection over Union (IoU) is a metrics used in both, object detection and segmentation problems.\n",
        "\n",
        " It is defined as the ratio between the intersection area between the predicted mask and the ground truth mask, over the union between the two masks.\n",
        "\n",
        "![](https://miro.medium.com/max/300/0*kraYHnYpoJOhaMzq.png)\n",
        "\n",
        "\n",
        "*   IoU scores close to 1.0 indicate a high level of accuracy. The predicted and ground truth areas overlap perfectly, meaning the area of overlap equals the area of union.\n",
        "\n",
        "when the predicted and groud truth areas overlap eachother perfectly. In other words, if the area of overlap is the same of the area of union.\n",
        "\n",
        "*   Conversely, IoU scores close to 0 suggest poor accuracy, indicating little to no overlap between the predicted and ground truth regions.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bze7NbzvExZq"
      },
      "source": [
        "In the case of segmentation, the intersection over union is computed considering each individual pixel in the predicted segmentation mask and the ground truth mask.\n",
        "\n",
        "$$\n",
        "IoU_{pixel} = \\frac{\\sum_{i=1}^{N}(P_i \\cap G_i)}{\\sum_{i=1}^{N}(P_i \\cup G_i)}\n",
        "$$\n",
        "\n",
        "\n",
        "Where:\n",
        "\n",
        "\n",
        "*   *Pi* is the predicted segmentation mask at pixel i,\n",
        "*   *Gi* is the ground truth mask at pixel i,\n",
        "*   *∩* denotes the intersection operator,\n",
        "*   *∪* denotes the union operator, and\n",
        "*   *N* is the total number of pixels in the image.\n",
        "\n",
        "**Recap of accuracy formula:**\n",
        "$$\n",
        "Accuracy = \\frac{Number\\ of\\ right\\ predictions}{Total\\ number\\ of\\ predictions}\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RgYiEZeUn9aE"
      },
      "source": [
        "### What is a mask?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358
        },
        "id": "AJ1ooJRNefuB",
        "outputId": "cbddb989-562b-4dd1-d192-4c996ae13002"
      },
      "outputs": [],
      "source": [
        "import imageio.v2 as imageio\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "url=\"https://drive.google.com/uc?id=1vWgkE4_TUuBJhlsZv7JdURRCM_WoYNRl\"\n",
        "img = imageio.imread(url)#download a picture\n",
        "\n",
        "plt.imshow(img)\n",
        "plt.title('Original Mask')\n",
        "plt.axis('off')\n",
        "print(\"Img shape:\",img.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_KxF3C2XnWT"
      },
      "source": [
        "Another examples is the following text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358
        },
        "id": "KMlrebsWadtp",
        "outputId": "cf235561-4299-429c-b233-ef3fbb33ad71"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "text = 'Hello world!'\n",
        "my_color =  [255,0,0] #Red of rgb\n",
        "text_image = cv2.putText(img.copy(), text,(300, 200), fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=1.5, color=my_color, thickness=3)\n",
        "\n",
        "# Display the image with text\n",
        "plt.imshow(text_image)  # Matplotlib converts from greyscale to a colormap\n",
        "plt.axis('off')  # Hide axes\n",
        "plt.title('Image with a text')\n",
        "plt.show()\n",
        "print(\"Text Img shape:\",text_image.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 504
        },
        "id": "Oc6tDj5md-dh",
        "outputId": "6486ac6c-0ac0-455c-bfe2-111445c07350"
      },
      "outputs": [],
      "source": [
        "color_red = [255,0,0]\n",
        "mask = (text_image == color_red).all(axis=2).astype(np.uint8)\n",
        "print( mask )\n",
        "print(\"mask shape:\", mask.shape)\n",
        "#the mask is printable\n",
        "plt.imshow(mask)\n",
        "plt.title('Mask')\n",
        "plt.axis('off')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhyAKoJXng0J"
      },
      "source": [
        "Masking is the process of pinpointing pixel locations associated with each object or area. This action generates binary images with the same dimensions as the input. Here, the pixels corresponding to the object of interest are True (or 1), while the rest are False (or 0).\n",
        "\n",
        "The concept is something that can be found in several contexts:\n",
        "\n",
        "*   A ground truth can be a mask\n",
        "*   An attention meccanism of a Transformers can be a mask.\n",
        "*   Data Augmentation techinques such as Partial Erasing or Occlusion are based on mask.\n",
        "*   A loss function can be modified trough a mask.\n",
        "*   The dropout can be implemented as a mask."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJRAQaEMpUKB"
      },
      "source": [
        "#Image Segmentation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQflTasopq7C"
      },
      "source": [
        "\n",
        "This is generally done with a Convolutional Neural Network that act as an image-to-image transform, mapping each pixel of $x$ to the corresponding class.\n",
        "\n",
        "*Remind:* Given an image $x \\in \\mathbb{R}^{m \\times n \\times c}$, an image-to-image map is a function $f: \\mathbb{R}^{m \\times n \\times c} \\to \\mathbb{R}^{m' \\times n' \\times c'}$. In our situation, $m = m'$, $n = n'$ and $c = c'$. An image-to-image map is required to do segmentation and some image processing tasks, but not for classification or object detection. \\\\\n",
        "\n",
        "Image-to-image maps are usually implemented by some variant of a Fully Convolutional Neural Network (FNN) design (e.g. ResNet, Autoencoders, ...). See https://heartbeat.comet.ml/a-2019-guide-to-semantic-segmentation-ca8242f5a7fc for details."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Abo48tFBqDQ5"
      },
      "source": [
        "![Image Segmentation](https://drive.google.com/uc?id=1A2zksYq9ehq5ghNKl2sOZfkochbbgFDG )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3k3KsJA5pqG5"
      },
      "source": [
        "**Semantic segmentation** separates different classes at the pixel level but does not separate different instances of the same class. The **Object detection** separates instances but provides only a crude approximation of the instance shape (the box).The task of **Instance segmentation** lays at the intersection of the two. It can be defined as the task of detecting all instances of the objects of interest in an image and classifying them; and segmenting them from the background at the pixel level.\n",
        "\n",
        "![Image Segmentation](https://drive.google.com/uc?id=1xO189Tlv8GbE86sncajEn6LmkE3NTu6l )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-FGBAXn2m8I"
      },
      "source": [
        "###Some Datasets:\n",
        "\n",
        "*   Pascal2:11540 images,6,929 segmentation masks and 20 categories.\n",
        "     http://host.robots.ox.ac.uk/pascal/VOC/voc2012/index.html:\n",
        "\n",
        "*   ADEK20:images 25K+2K and 150 categories.\n",
        "     https://groups.csail.mit.edu/vision/datasets/ADE20K/\n",
        "*   Cityscapes Dataset: https://www.cityscapes-dataset.com/\n",
        "*   Mapillar Vistas: https://www.mapillary.com/dataset/vistas\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IUF0M09rrBv5"
      },
      "outputs": [],
      "source": [
        "!pip install keras_cv -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J488Yr2xtRoF"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import os\n",
        "from pathlib import Path\n",
        "import keras\n",
        "import keras_cv\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# Image Config\n",
        "HEIGHT = 64\n",
        "WIDTH = 64\n",
        "NUM_CLASSES = 3\n",
        "AUTOTUNE = tf.data.AUTOTUNE #optimize hw performance automatically\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "dir=\"./model_weights\"\n",
        "if not Path(dir).exists():\n",
        "  os.mkdir(dir)\n",
        "  print(\"Folder was created\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kw2dC99BqeIR",
        "outputId": "355a45e0-0bdf-4b9d-d7f2-b9f821898d7a"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2e5RIkwYkZj"
      },
      "source": [
        "In this tutorial we are going to introduce [The Oxford-IIIT Pet Dataset](https://www.robots.ox.ac.uk/~vgg/data/pets/).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CMBUqV6_t5o-",
        "outputId": "52aa1ed2-8531-4fe8-c186-64da18cdca07"
      },
      "outputs": [],
      "source": [
        "#Download the datasets\n",
        "tfds.disable_progress_bar()\n",
        "orig_train_ds, orig_val_ds = tfds.load(\n",
        "    name=\"oxford_iiit_pet\",\n",
        "    split=[\"train+test[:80%]\", \"test[80%:]\"],\n",
        ")\n",
        "\n",
        "# Calcola il numero di elementi in ciascun dataset\n",
        "num_train_elements = tf.data.experimental.cardinality(orig_train_ds).numpy()\n",
        "num_val_elements = tf.data.experimental.cardinality(orig_val_ds).numpy()\n",
        "\n",
        "print(\"Training set length:\", num_train_elements)\n",
        "print(\"Test set length:\", num_val_elements)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T4mFeA3fuUfc"
      },
      "outputs": [],
      "source": [
        "#data preprocessing\n",
        "def rescale_images_and_correct_masks( inputs):\n",
        "  return {\n",
        "    \"images\": tf.cast(inputs[\"image\"], dtype=tf.float32) / 255.0, #normalization\n",
        "    \"segmentation_masks\": inputs[\"segmentation_mask\"] - 1, #put all values as 0-based.\n",
        "\n",
        "  }\n",
        "#now the label of the ground truth pixels are 0 for pet, 1 for borders, 2 for background\n",
        "\n",
        "\n",
        "#utility function\n",
        "def unpackage_inputs(inputs):\n",
        "    images = inputs[\"images\"]\n",
        "    segmentation_masks = inputs[\"segmentation_masks\"]\n",
        "    return images, segmentation_masks\n",
        "\n",
        "\n",
        "train_ds = orig_train_ds.map( rescale_images_and_correct_masks, num_parallel_calls=AUTOTUNE )\n",
        "val_ds = orig_val_ds.map(rescale_images_and_correct_masks, num_parallel_calls=AUTOTUNE)\n",
        "\n",
        "resize_fn = keras_cv.layers.Resizing( HEIGHT, WIDTH )\n",
        "\n",
        "resized_val_ds = (\n",
        "    val_ds.map(resize_fn, num_parallel_calls=AUTOTUNE)\n",
        "    .batch(BATCH_SIZE)\n",
        "    .map(unpackage_inputs)\n",
        "    .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        ")\n",
        "\n",
        "resized_train_ds = (\n",
        "    train_ds.map(resize_fn, num_parallel_calls=AUTOTUNE)\n",
        "    .batch(BATCH_SIZE)\n",
        "    .map(unpackage_inputs)\n",
        "    .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 869
        },
        "id": "w3zalPSAuiLp",
        "outputId": "8ec24f39-0bfb-4f8b-d275-fff9a59c647e"
      },
      "outputs": [],
      "source": [
        "plot_train_ds = train_ds.map(unpackage_inputs).ragged_batch(4)\n",
        "#resized_train_ds\n",
        "images, segmentation_masks = next(iter(plot_train_ds.take(1)))\n",
        "\n",
        "print( f\"Image Shape: {images.shape}\"  )\n",
        "print( f\"Segmentation Mask Shape: {segmentation_masks.shape}\"  )\n",
        "\n",
        "keras_cv.visualization.plot_segmentation_mask_gallery(\n",
        "    images,\n",
        "    value_range=(0, 1),\n",
        "    num_classes=3,\n",
        "    y_true=segmentation_masks,\n",
        "    y_pred=None,\n",
        "    scale=8,\n",
        "    rows=2,\n",
        "    cols=2,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qMSgTsjHvM8N"
      },
      "outputs": [],
      "source": [
        "#Create a callback\n",
        "\n",
        "# Taking a batch of test inputs to measure model's progress.\n",
        "test_images, test_masks = next(iter(resized_val_ds))\n",
        "\n",
        "class DisplayCallback(keras.callbacks.Callback):\n",
        "    def __init__(self, epoch_interval=None):\n",
        "        self.epoch_interval = epoch_interval\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        if self.epoch_interval and epoch % self.epoch_interval == 0:\n",
        "\n",
        "            self.model.save_weights(save_path)\n",
        "\n",
        "            pred_masks = self.model.predict(test_images)\n",
        "            pred_masks = tf.math.argmax(pred_masks, axis=-1)\n",
        "            pred_masks = pred_masks[..., tf.newaxis] #add a new dimension at the end of pred_masks.\n",
        "            # ... is a placeholder for dimensions\n",
        "\n",
        "            # Randomly select an image from the test batch\n",
        "            random_index = random.randint(0, BATCH_SIZE - 1)\n",
        "            random_image = test_images[random_index]\n",
        "            random_pred_mask = pred_masks[random_index]\n",
        "            random_true_mask = test_masks[random_index]\n",
        "\n",
        "            fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(10, 5))\n",
        "            ax[0].imshow(random_image)\n",
        "            ax[0].set_title(f\"Image: {epoch:03d}\")\n",
        "\n",
        "            ax[1].imshow(random_true_mask)\n",
        "            ax[1].set_title(f\"Ground Truth Mask: {epoch:03d}\")\n",
        "\n",
        "            ax[2].imshow(random_pred_mask)\n",
        "            ax[2].set_title(f\"Predicted Mask: {epoch:03d}\", )\n",
        "\n",
        "            plt.show()\n",
        "            plt.close()\n",
        "\n",
        "early_stopping = keras.callbacks.EarlyStopping(\n",
        "    monitor=\"val_accuracy\",\n",
        "    restore_best_weights=True,\n",
        "    start_from_epoch=0,\n",
        "    patience=3\n",
        ")\n",
        "\n",
        "\n",
        "callbacks = [DisplayCallback(5),early_stopping]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h0H9EcFHd2NW"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLhj7r6tv5q6"
      },
      "source": [
        "## UNet\n",
        "\n",
        "Maybe the most known network architecture used for segmentation is the UNet.\n",
        "\n",
        "![](https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/u-net-architecture.png)\n",
        "\n",
        "its architecture is based on the formula that you studied to compute the receptive field of a convolutional neural network:\n",
        "\n",
        "$$\n",
        "D' = S(D-1) + K\n",
        "$$\n",
        "\n",
        "where $D'$ is the receptive field of the previous layer, $D$ is the receptive field on the following layer, $S$ is the stride and $K$ is the kernel size. \\\\\n",
        "\n",
        "A consequence of this formula is that the receptive field increases exponentially while moving down, linearly while moving right. \\\\\n",
        "\n",
        "The drawback of downsampling, which is the information loss, is solved by UNet by adding skip connections, that also act as training stabilizer. \\\\\n",
        "\n",
        "Note that, at every downsampling (which in this case is implemented as a MaxPooling2D layer), the number of filters double, to reduce the impact of the dimensionality loss (the total number of pixel after downsampling is divided by 4)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DZzheTANvYVh",
        "outputId": "70b50524-521a-48c3-d884-ea401cd919a4"
      },
      "outputs": [],
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, Concatenate\n",
        "\n",
        "def down_block(inputs, filters, kernel_size=(3, 3), padding='same', activation='relu'):\n",
        "    conv = Conv2D(filters, kernel_size, activation=activation, padding=padding)(inputs)\n",
        "    conv = Conv2D(filters, kernel_size, activation=activation, padding=padding)(conv)\n",
        "    pool = MaxPooling2D(pool_size=(2, 2))(conv)\n",
        "    return conv, pool\n",
        "\n",
        "def up_block(inputs, skip, filters, kernel_size=(3, 3), padding='same', activation='relu'):\n",
        "    up = UpSampling2D(size=(2, 2))(inputs)\n",
        "    concat = Concatenate()([up, skip])\n",
        "    conv = Conv2D(filters, kernel_size, activation=activation, padding=padding)(concat)\n",
        "    conv = Conv2D(filters, kernel_size, activation=activation, padding=padding)(conv)\n",
        "    return conv\n",
        "\n",
        "\n",
        "def unet(img_size=(256, 256, 1), num_classes=1):\n",
        "    inputs = Input(shape=img_size + (3,))\n",
        "\n",
        "    #Down Blocks\n",
        "    conv1, pool1 = down_block(inputs, 64)\n",
        "    conv2, pool2 = down_block(pool1, 128)\n",
        "    conv3, pool3 = down_block(pool2, 256)\n",
        "\n",
        "    # Bottleneck\n",
        "    conv4 = Conv2D(512, (3, 3), activation='relu', padding='same')(pool3)\n",
        "    conv4 = Conv2D(512, (3, 3), activation='relu', padding='same')(conv4)\n",
        "\n",
        "    #Up Blocks\n",
        "    conv5 = up_block(conv4, conv3, 256)\n",
        "    conv6 = up_block(conv5, conv2, 128)\n",
        "    conv7 = up_block(conv6, conv1, 64)\n",
        "\n",
        "    # Add a per-pixel classification layer\n",
        "    outputs = keras.layers.Conv2D(num_classes, 3, activation=\"softmax\", padding=\"same\")(conv7)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=outputs, name=\"unet\")\n",
        "    return model\n",
        "\n",
        "\n",
        "# Build model\n",
        "model = unet(img_size=(HEIGHT, WIDTH), num_classes=NUM_CLASSES)\n",
        "save_path=f\"./model_weights/weights_img{HEIGHT}x{WIDTH}.weights.h5\"\n",
        "if tf.test.gpu_device_name():\n",
        "    print('TensorFlow is using GPU/TPU')\n",
        "else:\n",
        "    print('TensorFlow is using CPU')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a-5pjldNvc_z",
        "outputId": "8ac64d9d-5856-48da-db0c-2928ad3d5af4"
      },
      "outputs": [],
      "source": [
        "# Training Config\n",
        "EPOCHS = 30\n",
        "LEARNING_RATE = 1e-3\n",
        "\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(LEARNING_RATE),\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n",
        "\n",
        "try:\n",
        "    model.load_weights(save_path)\n",
        "    print(\"The model was loaded\")\n",
        "except Exception as e:\n",
        "    print(f\"No weights loaded:\\n{e}\")\n",
        "\n",
        "\n",
        "# Train the model, doing validation at the end of each epoch.\n",
        "history = model.fit(\n",
        "    resized_train_ds,\n",
        "    epochs=EPOCHS,\n",
        "    validation_data=resized_val_ds,\n",
        "    callbacks=callbacks,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZPx5xjmYvgo2"
      },
      "outputs": [],
      "source": [
        "model.save_weights(save_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r8bAMlgBqCTm"
      },
      "outputs": [],
      "source": [
        "# np.save(\"val_accuracy\", hist.history['val_accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 354
        },
        "id": "QsDRwGEFvhfy",
        "outputId": "baeafe7c-50dc-4bc1-bf11-5f46c547cf12"
      },
      "outputs": [],
      "source": [
        "model.load_weights(save_path)\n",
        "\n",
        "pred_masks = model.predict(test_images)\n",
        "pred_masks = tf.math.argmax(pred_masks, axis=-1)[..., None]\n",
        "\n",
        "keras_cv.visualization.plot_segmentation_mask_gallery(\n",
        "    test_images,\n",
        "    value_range=(0, 1),\n",
        "    num_classes=3,\n",
        "    y_true=test_masks,\n",
        "    y_pred=pred_masks,\n",
        "    scale=4,\n",
        "    rows=2,\n",
        "    cols=4,\n",
        ")\n",
        "\n",
        "#original source code at https://keras.io/examples/vision/oxford_pets_image_segmentation/#prediction-with-trained-model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 220
        },
        "id": "6KG8KIVTvuGC",
        "outputId": "7aedc29b-1d52-4124-b064-f6510f92999b"
      },
      "outputs": [],
      "source": [
        "# Check overfit\n",
        "loss_history = history.history['loss']\n",
        "val_loss_history = history.history['val_loss']\n",
        "\n",
        "acc_history = history.history['accuracy']\n",
        "val_acc_history = history.history['val_accuracy']\n",
        "\n",
        "plt.plot(loss_history)\n",
        "plt.plot(val_loss_history)\n",
        "plt.grid()\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Loss', 'Val Loss'])\n",
        "plt.title('Loss')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(acc_history)\n",
        "plt.plot(val_acc_history)\n",
        "plt.grid()\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Accuracy', 'Val Accuracy'])\n",
        "plt.title('Accuracy')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_-6fxVLvrYx"
      },
      "source": [
        "## Evaluation\n",
        "\n",
        "There are multiple metrics used to measure the quality of the segmentation. The most important are:\n",
        "\n",
        "* Accuracy\n",
        "* Intersection over Union (IoU)\n",
        "* Dice Coefficient\n",
        "\n",
        "### Accuracy\n",
        "The accuracy is simply defined by considering the segmentation as a pixel-by-pixel classification. \\\\\n",
        "\n",
        "### Intersection over Union\n",
        "The Intersection over Union (IoU) is pretty intuitive. It is defined as the ratio between the intersection area between the predicted mask and the ground truth mask, over the union between the two masks.\n",
        "\n",
        "![](https://miro.medium.com/max/300/0*kraYHnYpoJOhaMzq.png)\n",
        "\n",
        "By using that the mask is a binary image, it is trivial to compute both the intersection and the union (the latter, computed via the relationship:\n",
        "\n",
        "$$\n",
        "\\mu (A \\cup B) + \\mu (A \\cap B) = \\mu (A) + \\mu (B)\n",
        "$$\n",
        "\n",
        "where $\\mu(A)$ is defined to be the Area of A. \\\\\n",
        "\n",
        "Clearly, $IoU(y, y') \\in [0, 1]$, and $IoU(y, y') = 1$ in the best case, where $y$ and $y'$ overlap perfectly, and $IoU(y, y') = 0$ when they don't overlap."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jfZImd7mNhjq",
        "outputId": "364ca913-7ea2-474a-c65d-59cf72c3cc37"
      },
      "outputs": [],
      "source": [
        "# import keras.ops as K\n",
        "import keras.backend as K\n",
        "\n",
        "def iou_coeff(y_true, y_pred):\n",
        "    print(\"y_true shape\", y_true.shape)\n",
        "    print(\"y_pred shape\", y_pred.shape)\n",
        "    smooth = 1\n",
        "    intersection = K.sum(K.abs(y_true * y_pred), axis=[1,2,3])\n",
        "    union = K.sum(y_true,[1,2,3]) + K.sum(y_pred,[1,2,3]) - intersection\n",
        "    iou = K.mean((intersection + smooth) / (union + smooth), axis=0)\n",
        "    return iou\n",
        "\n",
        "def evaluate_model(model, validation_dataset, fun):\n",
        "    y_pred = model.predict(validation_dataset)\n",
        "    y_true = np.concatenate([y for x, y in validation_dataset], axis=0)\n",
        "    y_pred = y_pred.astype('float32')\n",
        "    y_true = y_true.astype('float32')\n",
        "    evaluation_result = fun(y_true, y_pred)\n",
        "    return evaluation_result\n",
        "\n",
        "iou = evaluate_model(model, resized_val_ds, iou_coeff)\n",
        "print(\"Pixel wise IoU:\", iou.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iAxWAZffNL_a"
      },
      "source": [
        "Try to experiment by yourself at home."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zceArwg8HgYo"
      },
      "source": [
        "Some suggested readings:\n",
        "\n",
        "*  [Panoptic Segmentation](https://openaccess.thecvf.com/content_CVPR_2019/papers/Kirillov_Panoptic_Segmentation_CVPR_2019_paper.pdf)\n",
        "\n",
        "*    [Panoptic FeaturePyramidNetwork](https://arxiv.org/pdf/1901.02446.pdf)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
