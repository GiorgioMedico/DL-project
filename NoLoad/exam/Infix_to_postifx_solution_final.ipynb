{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "5b284d21",
      "metadata": {
        "id": "5b284d21"
      },
      "source": [
        "# Neural Translation: Infix to Postfix Notation\n",
        "\n",
        "The purpose of this project is to implement a neural network that performs the translation of mathematical formulae from traditional **infix notation**—where the operator appears between two operands—to **postfix** (also known as Reverse Polish Notation), where the operator follows the operands.\n",
        "\n",
        "Infix notation is the most commonly used in human-readable mathematics (e.g., `a + b`), but it is inherently ambiguous without additional syntactic aids such as parentheses or operator precedence rules.\n",
        "\n",
        "In contrast, postfix notation eliminates the need for parentheses entirely. The order of operations is explicitly encoded by the position of the operators relative to the operands, making it more suitable for stack-based evaluation and easier to parse programmatically.\n",
        "\n",
        "**Example:**\n",
        "\n",
        "Consider the ambiguous infix expression:\n",
        "`a + b * c`\n",
        "\n",
        "This expression can be parsed in two different ways:\n",
        "\n",
        "| Interpretation (Infix) | Equivalent Postfix |\n",
        "| ---------------------- | ------------------ |\n",
        "| `(a + b) * c`          | `a b + c *`        |\n",
        "| `a + (b * c)`          | `a b c * +`        |\n",
        "\n",
        "This project aims to learn such disambiguations and generate the correct postfix form from a given infix expression using a data-driven approach. To control the complexity, we restrict our dataset to formulae with a **maximum syntactic depth of 3** and require that all binary operations are **fully parenthesized**, like `(e1 op e2)`."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a938292e",
      "metadata": {
        "id": "a938292e"
      },
      "source": [
        "## 1. Setup and Imports\n",
        "\n",
        "First, we import the necessary libraries. We'll use `numpy` for numerical operations, `tensorflow` and `keras` for building and training the model, `matplotlib` for plotting, and `random` for data generation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "471f66ca",
      "metadata": {
        "id": "471f66ca"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.models import load_model\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "27f632e0",
      "metadata": {
        "id": "27f632e0"
      },
      "source": [
        "### 1.1 Download the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22675ffa",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "22675ffa",
        "outputId": "26ac7ab3-e0c0-49f4-b2f7-7057b3f8c097"
      },
      "outputs": [],
      "source": [
        "# import gdown\n",
        "# !gdown 1ocOwpYn0xYQwXIbPDIudi8gNTt3RbFKZ"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a43b284",
      "metadata": {
        "id": "7a43b284"
      },
      "source": [
        "## 2. Constants and Vocabulary\n",
        "\n",
        "We define the vocabulary for our expressions. This includes identifiers (`a, b, c, d, e`), operators (`+, -, *, /`), parentheses, and special tokens for sequence processing: `PAD` (padding), `SOS` (start of sequence), and `EOS` (end of sequence)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b2e1b85",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7b2e1b85",
        "outputId": "463fb09e-7c12-4cdd-efbe-43088a2143ad"
      },
      "outputs": [],
      "source": [
        "# -------------------- Constants --------------------\n",
        "OPERATORS = ['+', '-', '*', '/']\n",
        "IDENTIFIERS = list('abcde')\n",
        "SPECIAL_TOKENS = ['PAD', 'SOS', 'EOS']\n",
        "SYMBOLS = ['(', ')', '+', '-', '*', '/']\n",
        "VOCAB = SPECIAL_TOKENS + SYMBOLS + IDENTIFIERS + ['JUNK']\n",
        "\n",
        "token_to_id = {tok: i for i, tok in enumerate(VOCAB)}\n",
        "id_to_token = {i: tok for tok, i in token_to_id.items()}\n",
        "VOCAB_SIZE = len(VOCAB)\n",
        "PAD_ID = token_to_id['PAD']\n",
        "EOS_ID = token_to_id['EOS']\n",
        "SOS_ID = token_to_id['SOS']\n",
        "\n",
        "MAX_DEPTH = 3\n",
        "MAX_LEN = 4*2**MAX_DEPTH - 2\n",
        "MODEL_FILE = \"full_model.keras\"\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"INFIX TO POSTFIX NEURAL NETWORK TRANSLATION\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Vocabulary size: {VOCAB_SIZE}\")\n",
        "print(f\"Maximum sequence length: {MAX_LEN}\")\n",
        "print(f\"Vocabulary: {VOCAB}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ff58c6a",
      "metadata": {
        "id": "3ff58c6a"
      },
      "source": [
        "## 3. Data Generation and Preprocessing\n",
        "\n",
        "Here we define the functions to generate our dataset.\n",
        "- `generate_infix_expression`: Recursively creates fully parenthesized infix expressions up to a specified depth.\n",
        "- `tokenize`: Splits an expression string into a list of tokens.\n",
        "- `infix_to_postfix`: Converts a tokenized infix expression to its postfix equivalent using a standard stack-based algorithm.\n",
        "- `encode`: Converts a list of tokens into a sequence of integer IDs, adding `EOS` and padding to `MAX_LEN`.\n",
        "- `decode_sequence`: Reverts a sequence of IDs back to a human-readable string.\n",
        "- `generate_dataset`: Creates a complete dataset of (infix, postfix) pairs.\n",
        "- `shift_right`: Prepares the decoder input for teacher forcing by prepending the `SOS` token."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca4aec82",
      "metadata": {
        "id": "ca4aec82"
      },
      "outputs": [],
      "source": [
        "def generate_infix_expression(max_depth):\n",
        "    if max_depth == 0:\n",
        "        return random.choice(IDENTIFIERS)\n",
        "    elif random.random() < 0.5:\n",
        "        return generate_infix_expression(max_depth - 1)\n",
        "    else:\n",
        "        left = generate_infix_expression(max_depth - 1)\n",
        "        right = generate_infix_expression(max_depth - 1)\n",
        "        op = random.choice(OPERATORS)\n",
        "        return f'({left} {op} {right})'\n",
        "\n",
        "def tokenize(expr):\n",
        "    return [c for c in expr if c in token_to_id]\n",
        "\n",
        "def infix_to_postfix(tokens):\n",
        "    precedence = {'+': 1, '-': 1, '*': 2, '/': 2}\n",
        "    output, stack = [], []\n",
        "    for token in tokens:\n",
        "        if token in IDENTIFIERS:\n",
        "            output.append(token)\n",
        "        elif token in OPERATORS:\n",
        "            while stack and stack[-1] in OPERATORS and precedence[stack[-1]] >= precedence[token]:\n",
        "                output.append(stack.pop())\n",
        "            stack.append(token)\n",
        "        elif token == '(':\n",
        "            stack.append(token)\n",
        "        elif token == ')':\n",
        "            while stack and stack[-1] != '(':\n",
        "                output.append(stack.pop())\n",
        "            stack.pop()\n",
        "    while stack:\n",
        "        output.append(stack.pop())\n",
        "    return output\n",
        "\n",
        "def encode(tokens, max_len=MAX_LEN):\n",
        "    ids = [token_to_id[t] for t in tokens] + [EOS_ID]\n",
        "    return ids + [PAD_ID] * (max_len - len(ids))\n",
        "\n",
        "def decode_sequence(token_ids, id_to_token, pad_token='PAD', eos_token='EOS'):\n",
        "    \"\"\"\n",
        "    Converts a list of token IDs into a readable string by decoding tokens.\n",
        "    Stops at the first EOS token if present, and ignores PAD tokens.\n",
        "    \"\"\"\n",
        "    tokens = []\n",
        "    for token_id in token_ids:\n",
        "        token = id_to_token.get(token_id, '?')\n",
        "        if token == eos_token:\n",
        "            break\n",
        "        if token != pad_token:\n",
        "            tokens.append(token)\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "def generate_dataset(n, max_depth=MAX_DEPTH):\n",
        "    X, Y = [], []\n",
        "    for _ in range(n):\n",
        "        expr = generate_infix_expression(max_depth)\n",
        "        infix = tokenize(expr)\n",
        "        postfix = infix_to_postfix(infix)\n",
        "        X.append(encode(infix))\n",
        "        Y.append(encode(postfix))\n",
        "    return np.array(X), np.array(Y)\n",
        "\n",
        "def shift_right(seqs):\n",
        "    shifted = np.zeros_like(seqs)\n",
        "    shifted[:, 1:] = seqs[:, :-1]\n",
        "    shifted[:, 0] = SOS_ID\n",
        "    return shifted"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ef3918f",
      "metadata": {
        "id": "0ef3918f"
      },
      "source": [
        "### Dataset Inspection\n",
        "\n",
        "Let's generate a sample dataset and inspect a few examples to ensure our functions are working correctly. We'll look at the original infix expression, the target postfix expression, and the shifted decoder input used for teacher forcing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e4dc9df",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8e4dc9df",
        "outputId": "b9aa48c4-c4c2-411f-b8c0-fe61d08dcc11"
      },
      "outputs": [],
      "source": [
        "# Generate a sample dataset\n",
        "X_demo, Y_demo = generate_dataset(5)\n",
        "decoder_input_demo = shift_right(Y_demo)\n",
        "\n",
        "# Inspect a random sample\n",
        "i =  np.random.randint(5)\n",
        "print(f\"--- Sample {i} ---\")\n",
        "print(\"Infix         : \", decode_sequence(X_demo[i], id_to_token))\n",
        "print(\"Postfix (Target): \", decode_sequence(Y_demo[i], id_to_token))\n",
        "print(\"Decoder Input   : \", decode_sequence(decoder_input_demo[i], id_to_token))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d32c02de",
      "metadata": {
        "id": "d32c02de"
      },
      "source": [
        "## 4. Model Architecture\n",
        "\n",
        "### Constraints\n",
        "* You may use any architecture (decoder-only, encoder-decoder, or other).\n",
        "* The maximum number of parameters is **2 million**.\n",
        "* Beam search is not allowed for generation.\n",
        "\n",
        "We will use an **Encoder-Decoder** architecture with LSTMs.\n",
        "\n",
        "- **Encoder**: A Bidirectional LSTM processes the input infix sequence and compresses it into a context vector (the hidden states `h` and `c`). Using a bidirectional LSTM allows the encoder to capture information from both past and future tokens at each step.\n",
        "- **Decoder**: An LSTM uses the encoder's final context vector as its initial state. It then generates the output postfix sequence one token at a time. The decoder's input at each step is the previously generated token (for inference) or the ground-truth token (for training via teacher forcing).\n",
        "- **Parameter Efficiency**: To stay under the 2 million parameter limit, we use a single Bidirectional LSTM in the encoder and a single LSTM in the decoder with moderate hidden dimensions. Dropout is applied to prevent overfitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a52f5e9",
      "metadata": {
        "id": "8a52f5e9"
      },
      "outputs": [],
      "source": [
        "def create_efficient_encoder_decoder_model(vocab_size=VOCAB_SIZE, max_len=MAX_LEN, embedding_dim=80, hidden_dim=144):\n",
        "    \"\"\"\n",
        "    Encoder-Decoder LSTM model optimized for parameter efficiency\n",
        "    Target: Under 2M parameters while maintaining strong performance\n",
        "    \"\"\"\n",
        "\n",
        "    # Encoder\n",
        "    encoder_input = layers.Input(shape=(max_len,), name='encoder_input')\n",
        "    encoder_embedding = layers.Embedding(vocab_size, embedding_dim)(encoder_input)\n",
        "\n",
        "    # Single bidirectional LSTM for encoder efficiency\n",
        "    encoder_lstm = layers.Bidirectional(\n",
        "        layers.LSTM(hidden_dim//2, return_state=True, dropout=0.04, recurrent_dropout=0.04))(encoder_embedding)\n",
        "\n",
        "    encoder_outputs = encoder_lstm[0]\n",
        "    encoder_state_h = layers.Concatenate()([encoder_lstm[1], encoder_lstm[3]])\n",
        "    encoder_state_c = layers.Concatenate()([encoder_lstm[2], encoder_lstm[4]])\n",
        "    encoder_states = [encoder_state_h, encoder_state_c]\n",
        "\n",
        "    # Decoder\n",
        "    decoder_input = layers.Input(shape=(max_len,), name='decoder_input')\n",
        "    decoder_embedding = layers.Embedding(vocab_size, embedding_dim)(decoder_input)\n",
        "\n",
        "    # Single LSTM decoder\n",
        "    decoder_lstm = layers.LSTM(hidden_dim, return_sequences=True, return_state=True, dropout=0.04, recurrent_dropout=0.04)\n",
        "    decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
        "\n",
        "    # Output projection\n",
        "    dense_intermediate = layers.Dense(hidden_dim//2, activation='relu')(decoder_outputs)\n",
        "    dense_intermediate = layers.Dropout(0.3)(dense_intermediate)\n",
        "    output = layers.Dense(vocab_size, activation='softmax')(dense_intermediate)\n",
        "\n",
        "    model = models.Model(inputs=[encoder_input, decoder_input], outputs=output)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5930c246",
      "metadata": {
        "id": "5930c246"
      },
      "source": [
        "## 5. Training Strategy\n",
        "\n",
        "We define a comprehensive training function that encapsulates dataset generation, model compilation, and the training loop.\n",
        "\n",
        "- **Model Compilation**: The model is compiled with the `Adam` optimizer, which is a robust choice for NLP tasks. We use `sparse_categorical_crossentropy` as the loss function because our targets are integer-encoded tokens. Gradient clipping (`clipnorm`) is used to stabilize training.\n",
        "- **Callbacks**:\n",
        "  - `EarlyStopping`: Halts training if the validation loss does not improve for a set number of epochs (`patience`), and restores the weights from the best epoch.\n",
        "  - `ReduceLROnPlateau`: Reduces the learning rate when the validation loss plateaus, allowing the model to find a better minimum."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32ee8aee",
      "metadata": {
        "id": "32ee8aee"
      },
      "outputs": [],
      "source": [
        "def create_and_compile_model():\n",
        "    \"\"\"Create and compile the model, ensuring parameter count is under 2M\"\"\"\n",
        "    model = create_efficient_encoder_decoder_model()\n",
        "\n",
        "    param_count = model.count_params()\n",
        "    print(f\"\\nModel parameter count: {param_count:,}\")\n",
        "\n",
        "    # Compile with appropriate loss and metrics\n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate=0.003, clipnorm=1.0),\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    return model\n",
        "\n",
        "\n",
        "def train_model(train_size=10000, val_size=1000, batch_size=64, epochs=50,\n",
        "                learning_rate=0.003, clipnorm=1.0, early_stopping_patience=8,\n",
        "                lr_reduction_factor=0.7, lr_reduction_patience=3, min_lr=1e-7,\n",
        "                verbose=1):\n",
        "\n",
        "    # Create separators\n",
        "    main_separator = \"=\" * 60\n",
        "    sub_separator = \"-\" * 40\n",
        "\n",
        "    print(f\"\\n{main_separator}\")\n",
        "    print(\"TRAINING PHASE\")\n",
        "    print(main_separator)\n",
        "\n",
        "    print(\"\\nGenerating datasets...\")\n",
        "    print(f\"  Training samples: {train_size:,}\")\n",
        "    print(f\"  Validation samples: {val_size:,}\")\n",
        "\n",
        "    X_train, Y_train = generate_dataset(train_size)\n",
        "    decoder_input_train = shift_right(Y_train)\n",
        "\n",
        "    X_val, Y_val = generate_dataset(val_size)\n",
        "    decoder_input_val = shift_right(Y_val)\n",
        "\n",
        "    # Model creation section\n",
        "    print(f\"\\n{sub_separator}\")\n",
        "    print(\"MODEL ARCHITECTURE\")\n",
        "    print(sub_separator)\n",
        "    model = create_and_compile_model()\n",
        "    model.summary()\n",
        "\n",
        "    print(f\"\\n{sub_separator}\")\n",
        "    print(\"TRAINING CONFIGURATION\")\n",
        "    print(sub_separator)\n",
        "\n",
        "    # Create callbacks with configurable parameters\n",
        "    callbacks = [\n",
        "        EarlyStopping(\n",
        "            monitor='val_loss',\n",
        "            patience=early_stopping_patience,\n",
        "            restore_best_weights=True,\n",
        "            verbose=verbose\n",
        "        ),\n",
        "        ReduceLROnPlateau(\n",
        "            monitor='val_loss',\n",
        "            factor=lr_reduction_factor,\n",
        "            patience=lr_reduction_patience,\n",
        "            min_lr=min_lr,\n",
        "            verbose=verbose\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    # Display all configuration parameters\n",
        "    print(f\"Optimizer: Adam (lr={learning_rate}, clipnorm={clipnorm})\")\n",
        "    print(\"Loss function: sparse_categorical_crossentropy\")\n",
        "    print(f\"Batch size: {batch_size}\")\n",
        "    print(f\"Max epochs: {epochs}\")\n",
        "    print(\"Callbacks:\")\n",
        "    print(f\"  - Early stopping: patience={early_stopping_patience}, monitor=val_loss\")\n",
        "    print(f\"  - Learning rate reduction: factor={lr_reduction_factor}, patience={lr_reduction_patience}\")\n",
        "    print(f\"  - Minimum learning rate: {min_lr}\")\n",
        "\n",
        "    print(f\"\\n{sub_separator}\")\n",
        "    print(\"TRAINING EXECUTION\")\n",
        "    print(sub_separator)\n",
        "\n",
        "    # Training with all configurable parameters\n",
        "    history = model.fit(\n",
        "        [X_train, decoder_input_train], Y_train,\n",
        "        batch_size=batch_size,\n",
        "        epochs=epochs,\n",
        "        validation_data=([X_val, decoder_input_val], Y_val),\n",
        "        callbacks=callbacks,\n",
        "        verbose=verbose\n",
        "    )\n",
        "\n",
        "    # Training analysis section\n",
        "    print(f\"\\n{sub_separator}\")\n",
        "    print(\"TRAINING RESULTS\")\n",
        "    print(sub_separator)\n",
        "\n",
        "    # Extract training metrics\n",
        "    final_train_acc = history.history['accuracy'][-1]\n",
        "    final_val_acc = history.history['val_accuracy'][-1]\n",
        "    final_train_loss = history.history['loss'][-1]\n",
        "    final_val_loss = history.history['val_loss'][-1]\n",
        "    best_val_acc = max(history.history['val_accuracy'])\n",
        "    epochs_trained = len(history.history['loss'])\n",
        "\n",
        "    # Display results with improved formatting\n",
        "    print(f\"Training completed after {epochs_trained} epochs\")\n",
        "    print(f\"Final training accuracy: {final_train_acc:.5f}\")\n",
        "    print(f\"Final validation accuracy: {final_val_acc:.5f}\")\n",
        "    print(f\"Best validation accuracy: {best_val_acc:.5f}\")\n",
        "    print(f\"Final training loss: {final_train_loss:.5f}\")\n",
        "    print(f\"Final validation loss: {final_val_loss:.5f}\")\n",
        "\n",
        "    print(f\"\\nSaving full model to {MODEL_FILE}...\")\n",
        "    model.save(MODEL_FILE)\n",
        "    print(\"Model saved successfully.\")\n",
        "\n",
        "    return model, history"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c00c7727",
      "metadata": {
        "id": "c00c7727"
      },
      "source": [
        "## 6. Inference and Evaluation\n",
        "\n",
        "### Autoregressive Decoding\n",
        "During inference, we don't have the ground-truth sequence to feed to the decoder. Instead, we must generate the output **autoregressively**:\n",
        "1.  Feed the `SOS` token as the initial input to the decoder.\n",
        "2.  Predict the next token in the sequence.\n",
        "3.  Use this predicted token as the input for the next time step.\n",
        "4.  Repeat until an `EOS` token is generated or the maximum length is reached.\n",
        "\n",
        "This is a form of **greedy decoding**, as we always choose the token with the highest probability at each step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6bc9337",
      "metadata": {
        "id": "e6bc9337"
      },
      "outputs": [],
      "source": [
        "def autoregressive_decode(model, encoder_input, max_length=MAX_LEN):\n",
        "    \"\"\"\n",
        "    Autoregressive generation for an encoder-decoder model.\n",
        "    \"\"\"\n",
        "    encoder_input = np.expand_dims(encoder_input, 0)\n",
        "\n",
        "    # Initialize decoder input with SOS token\n",
        "    decoder_input = np.zeros((1, max_length), dtype=np.int32)\n",
        "    decoder_input[0, 0] = SOS_ID\n",
        "\n",
        "    for i in range(1, max_length):\n",
        "        # Get predictions from the model\n",
        "        predictions = model.predict([encoder_input, decoder_input], verbose=0)\n",
        "\n",
        "        # Select the next token (greedy decoding)\n",
        "        next_token_id = np.argmax(predictions[0, i-1, :])\n",
        "\n",
        "        # Stop if the EOS token is generated\n",
        "        if next_token_id == EOS_ID:\n",
        "            break\n",
        "\n",
        "        # Update the decoder input for the next step\n",
        "        decoder_input[0, i] = next_token_id\n",
        "\n",
        "    return decoder_input[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34403448",
      "metadata": {
        "id": "34403448"
      },
      "source": [
        "### Evaluation Metric: Prefix Accuracy\n",
        "\n",
        "We evaluate a generated sequence `y_pred` using **prefix accuracy**. This measures the length of the initial prefix of `y_pred` that exactly matches the ground truth `y_true`.\n",
        "\n",
        "$$ \\text{Score} = \\frac{\\text{Length of matching prefix}}{\\max(\\text{len}(y_{\\text{true}}), \\text{len}(y_{\\text{pred}}))} $$\n",
        "\n",
        "This metric is more informative than an exact match (which is often 0) and captures how much of the sequence the model generated correctly before making a mistake."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43be86b1",
      "metadata": {
        "id": "43be86b1"
      },
      "outputs": [],
      "source": [
        "def prefix_accuracy_single(y_true, y_pred, id_to_token, eos_id=EOS_ID, verbose=False):\n",
        "    t_str = decode_sequence(y_true, id_to_token).split(' EOS')[0]\n",
        "    p_str = decode_sequence(y_pred, id_to_token).split(' EOS')[0]\n",
        "    t_tokens = t_str.strip().split()\n",
        "    p_tokens = p_str.strip().split()\n",
        "    max_len = max(len(t_tokens), len(p_tokens))\n",
        "\n",
        "    match_len = sum(x == y for x, y in zip(t_tokens, p_tokens))\n",
        "    score = match_len / max_len if max_len>0 else 0\n",
        "\n",
        "    if verbose:\n",
        "        print(\"TARGET :\", ' '.join(t_tokens))\n",
        "        print(\"PREDICT:\", ' '.join(p_tokens))\n",
        "        print(f\"PREFIX MATCH: {match_len}/{len(t_tokens)} → {score:.2f}\")\n",
        "\n",
        "    return score\n",
        "\n",
        "def test(model, no=20, rounds=10):\n",
        "    print(f\"Evaluating model performance on {no} expressions × {rounds} rounds...\")\n",
        "    rscores = []\n",
        "    for i in range(rounds):\n",
        "        print(f\"Round {i+1}/{rounds}...\")\n",
        "        X_test, Y_test = generate_dataset(no)\n",
        "        scores = []\n",
        "        for j in range(no):\n",
        "            encoder_input = X_test[j]\n",
        "            generated = autoregressive_decode(model, encoder_input)[1:]  # remove SOS\n",
        "            scores.append(prefix_accuracy_single(Y_test[j], generated, id_to_token))\n",
        "        round_mean = np.mean(scores)\n",
        "        rscores.append(round_mean)\n",
        "        print(f\"  Round {i+1} accuracy: {round_mean:.5f}\")\n",
        "\n",
        "    final_mean = np.mean(rscores)\n",
        "    final_std = np.std(rscores)\n",
        "    print(\"\\nEvaluation complete!\")\n",
        "    print(f\"Mean accuracy across all rounds: {final_mean:.5f}\")\n",
        "    print(f\"Standard deviation: {final_std:.5f}\")\n",
        "\n",
        "    return final_mean, final_std"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2655fe80",
      "metadata": {
        "id": "2655fe80"
      },
      "source": [
        "## 7. Visualization and Analysis\n",
        "\n",
        "We'll use these functions to analyze the results.\n",
        "- `plot_training_history`: Visualizes the training/validation accuracy and loss over epochs.\n",
        "- `demonstrate_model_performance`: Shows qualitative results by printing the model's predictions on a few random examples and calculating the prefix accuracy score for each."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36d6d0cb",
      "metadata": {
        "id": "36d6d0cb"
      },
      "outputs": [],
      "source": [
        "def plot_training_history(history):\n",
        "\n",
        "    # Extract history data\n",
        "    loss_history = history.history['loss']\n",
        "    val_loss_history = history.history['val_loss']\n",
        "    acc_history = history.history['accuracy']\n",
        "    val_acc_history = history.history['val_accuracy']\n",
        "\n",
        "    # Create figure with subplots\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "    # Loss plot\n",
        "    ax1.plot(loss_history, label='Training Loss', linewidth=2, color='blue')\n",
        "    ax1.plot(val_loss_history, label='Validation Loss', linewidth=2, color='red')\n",
        "    ax1.set_title('Loss During Training', fontsize=14, fontweight='bold')\n",
        "    ax1.set_xlabel('Epoch', fontsize=12)\n",
        "    ax1.set_ylabel('Loss', fontsize=12)\n",
        "    ax1.legend(fontsize=11)\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "\n",
        "    # Accuracy plot\n",
        "    ax2.plot(acc_history, label='Training Accuracy', linewidth=2, color='green')\n",
        "    ax2.plot(val_acc_history, label='Validation Accuracy', linewidth=2, color='orange')\n",
        "    ax2.set_title('Accuracy During Training', fontsize=14, fontweight='bold')\n",
        "    ax2.set_xlabel('Epoch', fontsize=12)\n",
        "    ax2.set_ylabel('Accuracy', fontsize=12)\n",
        "    ax2.legend(fontsize=11)\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Print training summary\n",
        "    epochs_trained = len(loss_history)\n",
        "    final_train_loss = loss_history[-1]\n",
        "    final_val_loss = val_loss_history[-1]\n",
        "    final_train_acc = acc_history[-1]\n",
        "    final_val_acc = val_acc_history[-1]\n",
        "    best_val_acc = max(val_acc_history)\n",
        "\n",
        "    print(f\"\\n{'-'*50}\")\n",
        "    print(\"TRAINING SUMMARY\")\n",
        "    print(f\"{'-'*50}\")\n",
        "    print(f\"Total epochs trained: {epochs_trained}\")\n",
        "    print(f\"Final training loss: {final_train_loss:.5f}\")\n",
        "    print(f\"Final validation loss: {final_val_loss:.5f}\")\n",
        "    print(f\"Final training accuracy: {final_train_acc:.5f}\")\n",
        "    print(f\"Final validation accuracy: {final_val_acc:.5f}\")\n",
        "    print(f\"Best validation accuracy: {best_val_acc:.5f}\")\n",
        "\n",
        "def demonstrate_model_performance(model, num_examples=10):\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(\"MODEL PERFORMANCE DEMONSTRATION\")\n",
        "    print(f\"{'='*70}\")\n",
        "\n",
        "    X_demo, Y_demo = generate_dataset(num_examples)\n",
        "\n",
        "    perfect_matches = 0\n",
        "    partial_matches = 0\n",
        "    total_score = 0\n",
        "\n",
        "    print(f\"Testing on {num_examples} randomly generated expressions:\\n\")\n",
        "\n",
        "    for i in range(num_examples):\n",
        "        encoder_input = X_demo[i]\n",
        "        target = Y_demo[i]\n",
        "        generated = autoregressive_decode(model, encoder_input)[1:]\n",
        "\n",
        "        infix_str = decode_sequence(encoder_input, id_to_token)\n",
        "        target_str = decode_sequence(target, id_to_token)\n",
        "        generated_str = decode_sequence(generated, id_to_token)\n",
        "\n",
        "        score = prefix_accuracy_single(target, generated, id_to_token)\n",
        "        total_score += score\n",
        "\n",
        "        if score == 1.0:\n",
        "            perfect_matches += 1\n",
        "            status = \"✓ PERFECT\"\n",
        "        elif score > 0.5:\n",
        "            partial_matches += 1\n",
        "            status = \"~ PARTIAL\"\n",
        "        else:\n",
        "            status = \"✗ POOR\"\n",
        "\n",
        "        print(f\"Example {i+1:2d}:\")\n",
        "        print(f\"  Input (Infix):     {infix_str}\")\n",
        "        print(f\"  Target (Postfix):  {target_str}\")\n",
        "        print(f\"  Generated:         {generated_str}\")\n",
        "        print(f\"  Score: {score:.3f}  [{status}]\")\n",
        "        print()\n",
        "\n",
        "    avg_score = total_score / num_examples\n",
        "\n",
        "    print(f\"{'-'*50}\")\n",
        "    print(\"DEMONSTRATION RESULTS\")\n",
        "    print(f\"{'-'*50}\")\n",
        "    print(f\"Average score: {avg_score:.3f}\")\n",
        "    print(f\"Perfect matches: {perfect_matches}/{num_examples} ({perfect_matches/num_examples*100:.1f}%)\")\n",
        "    print(f\"Partial matches: {partial_matches}/{num_examples} ({partial_matches/num_examples*100:.1f}%)\")\n",
        "    print(f\"Poor matches: {num_examples - perfect_matches - partial_matches}/{num_examples} ({(num_examples - perfect_matches - partial_matches)/num_examples*100:.1f}%)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50f78b2c",
      "metadata": {
        "id": "50f78b2c"
      },
      "source": [
        "## 8. Main Execution\n",
        "\n",
        "This is the main block where we tie everything together. It handles:\n",
        "1.  **Model Loading/Training**: It attempts to load a pre-trained model from `full_model.keras`. If the file doesn't exist (or if `load=False`), it will train a new model from scratch.\n",
        "2.  **Visualization**: If a new model was trained, it plots the training history.\n",
        "3.  **Demonstration**: It runs a qualitative demonstration of the model's performance.\n",
        "4.  **Final Evaluation**: It performs the final, formal evaluation by running the `test` function, which calculates the mean and standard deviation of prefix accuracy over 10 rounds.\n",
        "\n",
        "To force retraining of the model, set `FORCE_RETRAIN = True`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43624218",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "43624218",
        "outputId": "6e49067f-f9f0-4d55-ad20-a04d3ac1b2cd"
      },
      "outputs": [],
      "source": [
        "# --- Configuration --- #\n",
        "# Set to True to force retraining even if a model file exists.\n",
        "FORCE_RETRAIN = True # True\n",
        "\n",
        "model, history = None, None\n",
        "if not FORCE_RETRAIN and os.path.exists(MODEL_FILE):\n",
        "    print(f\"\\nLoading full model from {MODEL_FILE}...\")\n",
        "    model = models.load_model(MODEL_FILE)\n",
        "    print(\"Model loaded successfully.\")\n",
        "    model.summary()\n",
        "else:\n",
        "    if not FORCE_RETRAIN:\n",
        "        print(f\"\\nModel file '{MODEL_FILE}' not found. Training a new model.\")\n",
        "    else:\n",
        "        print(\"\\nForcing retraining of a new model.\")\n",
        "    # Training phase\n",
        "    model, history = train_model()\n",
        "\n",
        "# Visualize training history if training was performed\n",
        "if history:\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"TRAINING HISTORY VISUALIZATION\")\n",
        "    print(f\"{'='*60}\")\n",
        "    plot_training_history(history)\n",
        "\n",
        "# Demonstrate model performance on a few examples\n",
        "demonstrate_model_performance(model, num_examples=10)\n",
        "\n",
        "# Final, formal evaluation as specified\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"FINAL EVALUATION\")\n",
        "print(f\"{'='*60}\")\n",
        "result_mean, result_std = test(model, no=20, rounds=10)\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"FINAL RESULTS\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"Mean Prefix Accuracy Score: {result_mean:.5f}\")\n",
        "print(f\"Standard Deviation:         {result_std:.5f}\")\n",
        "print(f\"Model Parameters:           {model.count_params():,}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
