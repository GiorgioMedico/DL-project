{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ca4ef59-9645-433c-a575-dd28fe6f288f",
   "metadata": {},
   "source": [
    "# Text and Transformers lab\n",
    "In this lecture: a bit of text processing, loading text in keras, classification with 1DConvnet & transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98352700-91cd-4ffb-b433-378aebb53292",
   "metadata": {},
   "source": [
    "Let's start by reviewing a bit of text processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c221a1-f6d6-46d0-a750-79cae2b5cdcd",
   "metadata": {},
   "source": [
    "Unlike Images, Text has quite a few criticalities. Just to name a few:\n",
    "\n",
    "- High Dimensionality: Text data, especially when tokenized into individual words or n-grams, can lead to a very high-dimensional feature space. This can make models computationally expensive and increase the risk of overfitting.\n",
    "\n",
    "- Sparsity: Most text documents will only contain a tiny fraction of the words in a language, resulting in many zeros in the feature representation. This sparsity can make certain modeling techniques inefficient or infeasible.\n",
    "\n",
    "- Ambiguity and Polysemy: Many words in languages have multiple meanings based on context. For example, the word \"bank\" can mean the side of a river or a financial institution.\n",
    "\n",
    "- Synonymy: Different words can have similar meanings, like \"big\" and \"large\". This poses a challenge in identifying the true intent or sentiment behind texts.\n",
    "\n",
    "- Complex Dependencies: The meaning of a word can depend on its surrounding words, or even words much earlier in a text. Capturing long-term dependencies can be challenging.\n",
    "\n",
    "- Noisy Data: Text data, especially from sources like social media, can be noisy. They may contain typos, slang, non-standard grammar, or emoticons.\n",
    "\n",
    "- Handling of Out-of-Vocabulary (OOV) Words: In real-world applications, it's common to encounter words not seen during training. Handling OOV words is challenging.\n",
    "\n",
    "- Cultural and Temporal Dynamics: The way language is used can change based on cultural or temporal contexts, making it hard to generalize models across different cultures or time periods.\n",
    "\n",
    "- Multilingual Challenges: Building models that work across multiple languages or even dialects can be challenging, especially when resources for certain languages are scarce."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47407cd-5081-4cdc-b5d9-c033c98c4497",
   "metadata": {},
   "source": [
    "### some examples of regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3c48e7-07ef-4d86-adb1-0607eb9108c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "text = \"Hello, World!\"\n",
    "match = re.search(\"World\", text)\n",
    "if match:\n",
    "    print(\"Found:\", match.group())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f5584d-1bf9-49c3-b534-14843a256242",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"I am Free.\"\n",
    "match = re.search(\"[FT]ree\", text)\n",
    "if match:\n",
    "    print(\"Found:\", match.group())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3deeef33-e1b9-4e65-9a50-e88733c0f58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Order number: 12345\"\n",
    "match = re.search(\"\\d+\", text)  # + indicates one or more\n",
    "if match:\n",
    "    print(\"Found:\", match.group())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df4574b-037a-463f-b15c-d172e41a87b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Hello\"\n",
    "if re.match(\"^Hello\", text):  # ^ matches the start\n",
    "    print(\"Starts with 'Hello'\")\n",
    "if re.search(\"Hello$\", text):  # $ matches the end\n",
    "    print(\"Ends with 'Hello'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30166c73-2b96-4036-835e-625b3ddab920",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"apple,banana,orange\"\n",
    "fruits = re.split(\",\", text)\n",
    "print(fruits)  # ['apple', 'banana', 'orange']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86d1c62-1939-4ede-a1d8-251de6eabcc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"HELLO\"\n",
    "match = re.search(\"hello\", text, re.IGNORECASE)\n",
    "if match:\n",
    "    print(\"Found:\", match.group())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722231d8-f97e-460c-a10e-c28f00e96735",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"this is my email fabio.merizzi@unibo.it\"\n",
    "match = re.search(\"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,4}\", text)\n",
    "if match:\n",
    "    print(\"Found email:\", match.group())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866d76b4-22d6-47a5-ac06-fa2ed861880a",
   "metadata": {},
   "source": [
    "### Some basic ideas about text preprocessing\n",
    "Reducing the high dimensionality of text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d46e282-c2a7-446e-a428-2fe79d4ea287",
   "metadata": {},
   "source": [
    "A simple idea is to lowercase every word, casing has little meaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97425cf-5bba-47e2-852d-88f588eabfaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Hello here is some text, BYE\"\n",
    "# Lowercasing\n",
    "lowercased_text = text.lower()\n",
    "print(\"Lowercased Text:\", lowercased_text)\n",
    "# Tokenization\n",
    "tokens = lowercased_text.split()\n",
    "print(\"Tokens:\", tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db1e6c5-2ea1-46fa-a145-84c69dfccb8d",
   "metadata": {},
   "source": [
    "Punctuations as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44fc1524-ad93-407c-b6a1-aae072d248ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove punctuations\n",
    "import string\n",
    "text = \"Hello, World! who's speaking? Ah, it's you!\"\n",
    "clean_text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "print(\"Cleaned Text:\", clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba07d157-737f-4bd8-8b2e-6e8e49d5775f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#some words have less meaning\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "text = \"This is an example of removing stopwords from a sentence.\"\n",
    "tokens = text.split()\n",
    "filtered_tokens = [word for word in tokens if word.lower() not in stopwords.words('english')]\n",
    "print(\"Filtered Tokens:\", filtered_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86972964-e597-4302-ab43-84ab721b8d63",
   "metadata": {},
   "source": [
    "Some words have very similiar meaning but different coniugations, these may impair the classification, for addressing this problem we can employ Stemming and Lemming. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77cc85bc-a757-4622-aaaa-1cc2492888e2",
   "metadata": {},
   "source": [
    "Stemming truncates words by chopping off the ends of words using heuristic processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2459a1dc-51f3-4a5c-b691-20e703fa6fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "text = \"running runner runs run\"\n",
    "tokens = text.split()\n",
    "stemmed_tokens = [ps.stem(token) for token in tokens]\n",
    "print(\"Stemmed Tokens:\", stemmed_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea34d668-3c1c-4879-9743-b6aa401169bb",
   "metadata": {},
   "source": [
    "Lemming reduces words to their base or root form by considering the dictionary form of the word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589f6fe0-cc4d-4a41-931d-3a00c09db60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "text = \"geese mice swimming swam\"\n",
    "tokens = text.split()\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "print(\"Lemmatized Tokens:\", lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29aae5d3-82c4-4243-91cc-cc4f752171f3",
   "metadata": {},
   "source": [
    "### Encoding Text\n",
    "transforming text into numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e333984-dc1e-48ae-9b6c-5408d5ea7889",
   "metadata": {},
   "source": [
    "#### One hot encoding\n",
    "Practical, but only with very low cardinality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bace81da-3af4-41d0-bef6-834e4935508d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "words = [[\"hello\"], [\"this\"], [\"is\"], [\"the\"], [\"machine\"], [\"learning\"], [\"course\"]]\n",
    "\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "encoded_words = encoder.fit_transform(words)\n",
    "\n",
    "print(\"One-Hot Encoded Words:\\n\", encoded_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c43d30c-97ad-472a-9de2-b3dc3e758793",
   "metadata": {},
   "source": [
    "#### Label Encoding, great for transforming labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d88f953-3cc5-4955-8e1d-ab3093d4d42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "labels = [\"cat\", \"dog\", \"bird\", \"cat\", \"bird\"]\n",
    "encoder = LabelEncoder()\n",
    "encoded_labels = encoder.fit_transform(labels)\n",
    "print(encoded_labels) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbdfcf81-ed32-4aee-86a0-e7a5ee69beba",
   "metadata": {},
   "source": [
    "#### Bag of Words (BoW - TF) Encoding is essentially a histogram of word frequencies in a text document. For each document or text sample, count the number of occurrences of each word and represent the document as this count vector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8ad93e-2a90-42f8-8b15-232ef8fa342a",
   "metadata": {},
   "source": [
    " When to use it:\n",
    "\n",
    "    Document classification: BoW is popular for tasks like email spam classification or sentiment analysis, where the occurrence of certain words can be a strong indicator of the class.\n",
    "    Texts with different lengths: Since BoW leads to fixed-length vectors (the size of the vocabulary), it's useful when dealing with texts of varying lengths.\n",
    "    Simple models: For models like Naive Bayes, BoW can be very effective.\n",
    "\n",
    "Cons:\n",
    "\n",
    "    Loses all information about word order. \"This is good, not bad\" and \"This is bad, not good\" would have the same representation.\n",
    "    Like one-hot encoding, BoW can also lead to high-dimensional data with large vocabularies.\n",
    "    Doesn't capture semantic relationships between words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06db9ab-90d5-4197-848d-aa5dca7de081",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = [\n",
    "    \"Hello there this is the machine learning course, hello again\",\n",
    "    \"Welcome to the machine learning class.\"\n",
    "]\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "print(\"Vocabulary:\", vectorizer.get_feature_names_out())\n",
    "print(\"BoW Representation:\\n\", X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f83bd97-9add-4eaa-8d35-055e9e8493a8",
   "metadata": {},
   "source": [
    "#### TF-IDF\n",
    "TF-IDF is a statistical measure that evaluates the importance of a word in a document, relative to a collection of documents (often called a corpus). It's composed of two terms:\n",
    "\n",
    "Term Frequency (TF): Represents how often a term appears in a given document.\n",
    "\n",
    "$$TF(t,d)= \\frac{Number \\, of \\, times \\, term \\,t \\,appears \\,in \\,document \\,d}{Total\\, number \\,of \\,terms \\,in\\, document\\, d}$$\n",
    "\n",
    "\n",
    "Inverse Document Frequency (IDF): Represents the significance of the term across all documents in the corpus. It assigns more weight to terms that are rare across documents.\n",
    "\n",
    "$$IDF(t)= log⁡(\\frac{Total\\, number\\, of\\, documents}{Number\\, of \\,documents \\,containing\\, term\\, t})$$\n",
    "\n",
    "The TF-IDF value for a term in a document is then:\n",
    "$$TF-IDF(t,d)=TF(t,d)×IDF(t)$$\n",
    "\n",
    "Words with high TF-IDF scores are those that are important in a given document relative to the entire corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05685746-d9aa-4797-92be-3bc971de77db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "docs = [\n",
    "    \"I love machine learning.\",\n",
    "    \"Machine learning is challenging.\",\n",
    "    \"Python is a popular language for machine learning.\",\n",
    "]\n",
    "\n",
    "# Initialize a TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Compute TF-IDF values\n",
    "tfidf_matrix = vectorizer.fit_transform(docs)\n",
    "\n",
    "# Feature names (words in the vocabulary)\n",
    "features = vectorizer.get_feature_names_out()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fb7c52-b3ef-4b13-9378-9367b250f5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150db6dc-ec72-4025-b959-10b8f188085a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_matrix.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a980ca-5ced-4cc1-a8fe-c63040ea9025",
   "metadata": {},
   "source": [
    "#### What we really want: dense word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b008e7d-f6cf-4abb-af98-e3428d8fc3e5",
   "metadata": {},
   "source": [
    "The encodings methods mentioned above have two main drawbacks we need to solve. 1) The very high dimensionality of the embedding 2) similar words/concept are placed randomly in the encoding space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb49d3c1-0953-4ac1-be18-81c51e0f0a09",
   "metadata": {},
   "source": [
    "We well see in the following implementations how to solve both this issues via a neural embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3b595e-8c66-4a7a-bc20-e2d1cfdadbe1",
   "metadata": {},
   "source": [
    "### Loading text from file to keras dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74963cf7-3bc7-47b7-8f6a-10edfee879f5",
   "metadata": {},
   "source": [
    "IMDB movie reviews, one of the most common dataset for mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e90eb8-b915-4395-a867-7d57624de81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from keras import layers\n",
    "from keras import ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ad3a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "!tar -xf aclImdb_v1.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7bef29",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat aclImdb/train/pos/6248_7.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163bb51a-c54c-4c07-b183-0c9999b141bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -r aclImdb/train/unsup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff427f7-f904-4e6e-9b89-ff7e35fafac4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "raw_train_ds = keras.utils.text_dataset_from_directory(\n",
    "    \"aclImdb/train\",\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.5,\n",
    "    subset=\"training\",\n",
    "    seed=1337,\n",
    ")\n",
    "raw_val_ds = keras.utils.text_dataset_from_directory(\n",
    "    \"aclImdb/train\",\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.5,\n",
    "    subset=\"validation\",\n",
    "    seed=1337,\n",
    ")\n",
    "raw_test_ds = keras.utils.text_dataset_from_directory(\n",
    "    \"aclImdb/test\", batch_size=batch_size\n",
    ")\n",
    "\n",
    "print(f\"Number of batches in raw_train_ds: {raw_train_ds.cardinality()}\")\n",
    "print(f\"Number of batches in raw_val_ds: {raw_val_ds.cardinality()}\")\n",
    "print(f\"Number of batches in raw_test_ds: {raw_test_ds.cardinality()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d012f79-d849-40af-add7-0b295b7e19e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for text_batch, label_batch in raw_train_ds.take(1):\n",
    "    for i in range(5):\n",
    "        print(text_batch.numpy()[i])\n",
    "        print(label_batch.numpy()[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64294092-3c94-41b7-9b34-de768c4fd2a4",
   "metadata": {},
   "source": [
    "### Prepare text data in the keras framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e54b20-a4e6-4407-842f-42505b8a6afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "#Define custom a preprocessing procedure\n",
    "def custom_preprocessing(input_data):\n",
    "    lowercase = tf.strings.lower(input_data)\n",
    "    stripped_html = tf.strings.regex_replace(lowercase, \"<br />\", \" \")\n",
    "    no_punctuation = tf.strings.regex_replace(stripped_html, f\"[{re.escape(string.punctuation)}]\", \"\")\n",
    "    return no_punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f25585-91fe-4152-9683-018cf4740877",
   "metadata": {},
   "outputs": [],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb4befe-5e53-461b-9e30-ba838ef64b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's write our text vectorization layer \n",
    "# it will preprocess and map our words into a \n",
    "vectorize_layer = keras.layers.TextVectorization(\n",
    "    standardize=custom_preprocessing,\n",
    "    max_tokens=20000,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=500,\n",
    ")\n",
    "# Output can be int, multi_hot or tf_idf\n",
    "\n",
    "# Now that the vectorize_layer has been created, call `adapt` on a text-only\n",
    "# dataset to create the vocabulary. You don't have to batch, but for very large\n",
    "# datasets this means you're not keeping spare copies of the dataset in memory.\n",
    "\n",
    "# Let's make a text-only dataset (no labels):\n",
    "text_ds = raw_train_ds.map(lambda x, y: x)\n",
    "# Let's call `adapt`:\n",
    "vectorize_layer.adapt(text_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be051c5a-fa7b-46a1-9954-5c232bd2aa5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b = next(iter(raw_train_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf09d58-df42-4bcf-bbb3-f98ccacaa5a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0995ded0-d6c7-49bf-b97d-13e1402ab25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_text(text, label):\n",
    "    text = tf.expand_dims(text, -1)\n",
    "    return vectorize_layer(text), label\n",
    "\n",
    "\n",
    "# Vectorize the data.\n",
    "train_ds = raw_train_ds.map(vectorize_text)\n",
    "val_ds = raw_val_ds.map(vectorize_text)\n",
    "test_ds = raw_test_ds.map(vectorize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136d98ba-58a5-465e-9839-0c3823592ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b = next(iter(train_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c3aef9-6714-4f06-8c20-31a2303adb2a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc934cb6-eb2d-40ec-9923-49a1de89084f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = train_ds.cache().prefetch(buffer_size=10)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=10)\n",
    "test_ds = test_ds.cache().prefetch(buffer_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88766357-d1f2-4109-a848-bcb150b4d402",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A integer input for vocab indices.\n",
    "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "\n",
    "# Next, we add a layer to map those vocab indices into a space of dimensionality\n",
    "# 'embedding_dim'.\n",
    "x = layers.Embedding(20000,128)(inputs)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "\n",
    "# Conv1D + global max pooling\n",
    "x = layers.Conv1D(128, 7, padding=\"valid\", activation=\"relu\", strides=3)(x)\n",
    "x = layers.Conv1D(128, 7, padding=\"valid\", activation=\"relu\", strides=3)(x)\n",
    "x = layers.GlobalMaxPooling1D()(x)\n",
    "\n",
    "# We add a vanilla hidden layer:\n",
    "x = layers.Dense(128, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "\n",
    "# We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "predictions = layers.Dense(1, activation=\"sigmoid\", name=\"predictions\")(x)\n",
    "\n",
    "model = keras.Model(inputs, predictions)\n",
    "\n",
    "# Compile the model with binary crossentropy loss and an adam optimizer.\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38dcbe71-5eaa-4539-bcc1-3c5cc5915fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8e4fdb-5a46-40a0-9892-14ae4991f05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 15\n",
    "\n",
    "# Fit the model using the train and test datasets.\n",
    "model.fit(train_ds, validation_data=val_ds, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e6e225-5d51-49e6-a3f8-0b09d516bb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7b136d-a2d0-41b0-b593-a5ae1fc2719b",
   "metadata": {},
   "source": [
    "# Let's use a Transformer\n",
    "Transformer are based on the idea of attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ecbe0a-ecc0-4d12-bad5-298ec6fe9255",
   "metadata": {},
   "source": [
    "Understanding multi-head attention: The main idea behind multi-head attention is to split the attention mechanism into multiple \"heads.\" By doing this, the model can capture various aspects of the data in parallel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84c1e9d-df2b-4e5d-bac1-545e6eea6ea4",
   "metadata": {},
   "source": [
    "The Attention layer takes its input in the form of three parameters, known as the Query, Key, and Value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0fb3090-b826-4eb8-9838-594b4eda3c1a",
   "metadata": {},
   "source": [
    "$$Attention(Q,K,V) = Softmax(\\frac{QK^T}{\\sqrt(d_k)})*V$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ee68a6-c69c-4de3-8c83-c707b6625b8c",
   "metadata": {},
   "source": [
    "The interaction between Queries, Keys, and Values happens as follows in the attention mechanism:\n",
    "\n",
    "- Scoring: Each query vector is scored against all key vectors using a dot product, which measures how much each element (represented by a query) should attend to every other element (represented by keys).\n",
    "\n",
    "- Scaling: The scores are then scaled down by the square root of the dimension of the key vectors to help stabilize the gradients during learning.\n",
    "- Softmax: A softmax function is applied to the scaled scores, converting them into probabilities that sum to one. This softmaxed score represents the attention weights.\n",
    "- Application: These attention weights are then used to create a weighted sum of the value vectors, which forms the output for each position in the input sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f5540c-4cdb-42da-a095-c9a08d6f313a",
   "metadata": {},
   "source": [
    "Practically, values and keys are usually the same. In the case of self attention, Queries, Keys and Values are all the same. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f041998c-6829-4922-bb14-193f1940333b",
   "metadata": {},
   "source": [
    "So where is the learning actually happening? In the dense layers (Linear) before the computation of attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061b3b10-311b-41f6-88a5-4673436ca67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A possible implementation of multi head attention\n",
    "import tensorflow as tf\n",
    "\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, model_size, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.model_size = model_size\n",
    "        \n",
    "        assert model_size % self.num_heads == 0  # Ensure the model size is divisible by number of heads\n",
    "        \n",
    "        self.depth = model_size // self.num_heads\n",
    "        \n",
    "        self.wq = tf.keras.layers.Dense(model_size)  # Weight matrices for queries\n",
    "        self.wk = tf.keras.layers.Dense(model_size)  # Weight matrices for keys\n",
    "        self.wv = tf.keras.layers.Dense(model_size)  # Weight matrices for values\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(model_size)  # Final dense layer after concatenation\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        #Split the last dimension into (num_heads, depth).\n",
    "        #Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
    "        \n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, v, k, q):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "        \n",
    "        q = self.wq(q)  # Linear projection\n",
    "        k = self.wk(k)\n",
    "        v = self.wv(v)\n",
    "\n",
    "        q = self.split_heads(q, batch_size)  # Split heads\n",
    "        k = self.split_heads(k, batch_size)\n",
    "        v = self.split_heads(v, batch_size)\n",
    "\n",
    "        # Scaled dot-product attention\n",
    "        matmul_qk = tf.matmul(q, k, transpose_b=True)\n",
    "        dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "        scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "        # Apply the softmax is done on the last axis (seq_len_k) so that the scores add up to 1.\n",
    "        attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
    "        \n",
    "        output = tf.matmul(attention_weights, v)  # (batch_size, num_heads, seq_len_q, depth)\n",
    "\n",
    "        output = tf.transpose(output, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
    "        concat_output = tf.reshape(output, (batch_size, -1, self.model_size))  # (batch_size, seq_len_q, model_size)\n",
    "        \n",
    "        output = self.dense(concat_output)  # Pass through the final dense layer\n",
    "        \n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2423a5ad-015e-4f3d-b1ac-f885b589eb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super().__init__()\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        attn_output = self.att(inputs, inputs) # Multi head attention where Key, Value and Query are all the same\n",
    "        attn_output = self.dropout1(attn_output) # We add a dropout to reduce overfitting\n",
    "        out1 = self.layernorm1(inputs + attn_output) # We add a residual connection and layernorm the result \n",
    "        ffn_output = self.ffn(out1) # Feedforward network\n",
    "        ffn_output = self.dropout2(ffn_output) # a second dropout\n",
    "        return self.layernorm2(out1 + ffn_output) # a second residual connection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65485b5-5556-4444-ad6f-175267bc87cd",
   "metadata": {},
   "source": [
    "Transformers need both token embedding (the words) and position embedding (the token order) because the transformer architecture does not inherently process sequential data with an awareness of order or position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2d40b4-2ce2-46a1-b97b-8c3815b24f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "        # The embedding layer turns positive integers intodense vectors,\n",
    "        # (Words with similar meaning are close to each other)\n",
    "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        # get the number of tokens \n",
    "        maxlen = ops.shape(x)[-1]\n",
    "        # get all positions in order\n",
    "        positions = ops.arange(start=0, stop=maxlen, step=1)\n",
    "        # the the embedded positions\n",
    "        positions = self.pos_emb(positions)\n",
    "        # compute the token embeddings\n",
    "        x = self.token_emb(x)\n",
    "        # finally return the embedded tokens + the positions \n",
    "        return x + positions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697af448-6464-42fe-b70e-2d6085fb6ed5",
   "metadata": {},
   "source": [
    "Beware! this is a simple transformer approach for classification, tasks such as text generation, translation and so on requires a much more complex model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9072bf66-faa5-447f-906c-c9d754cd744c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 20000  # Only consider the top 20k words\n",
    "maxlen = 500 # max number of input tokens\n",
    "embed_dim = 32  # Embedding size for each token\n",
    "num_heads = 16  # Number of attention heads\n",
    "ff_dim = 64  # Hidden layer size in feed forward network inside transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e7a75c-6b79-45e5-9b7f-31b5f0898162",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "inputs = layers.Input(shape=(maxlen,)) # the input is a sequence of maxlen tokens (if not long enough is padded with zeros)\n",
    "embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim) # The embedding layer embed tokens and positions\n",
    "x = embedding_layer(inputs) \n",
    "transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim) \n",
    "x = transformer_block(x) # A transformer block process the data\n",
    "# What follows is a simple classifier \n",
    "x = layers.GlobalAveragePooling1D()(x)\n",
    "x = layers.Dropout(0.1)(x)\n",
    "x = layers.Dense(20, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.1)(x)\n",
    "outputs = layers.Dense(2, activation=\"softmax\")(x)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b995737c-f377-4c89-84db-6b563df475b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304e2d0a-115d-4dc1-baa1-12b61ae319f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = keras.optimizers.Adam(learning_rate=1e-4)\n",
    "model.compile(optimizer=opt, loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "history = model.fit(\n",
    "    train_ds, batch_size=32, epochs=10, validation_data=val_ds\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175df508-f724-45fd-877f-512cdd8238da",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(test_ds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
