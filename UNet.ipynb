{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eTSZKgkJbjhE"
      },
      "source": [
        "# Satellite image inpainting\n",
        "\n",
        "The project concerns image inpainting, which consists in filling in damaged or missing parts of an image to reconstruct a complete image.\n",
        "\n",
        "The dataset considered is the EuroSAT Tensorflow dataset based on Sentinel-2 satellite images, in the rgb version. This includes 27000 images, at 64x64 resolution.\n",
        "\n",
        "A portion of the image is randomly masked according to the procedure described below. The goal is to reconstruct the complete image.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e2Sqkh_BKQrs"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import cv2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LpIONE3ObCbg"
      },
      "source": [
        "###Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ds_train, ds_info = tfds.load(\n",
        "    'eurosat/rgb',\n",
        "    shuffle_files=False,\n",
        "    #as_supervised=True,  # Returns a tuple (img, label) instead of a dictionary {'image': img, 'label': label}\n",
        "    with_info=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PNzoRTX7X94j"
      },
      "outputs": [],
      "source": [
        "ds_train = ds_train['train']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9O9ncfeIYBE_"
      },
      "outputs": [],
      "source": [
        "ds_train = ds_train.shuffle(1000, seed = 42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LInPWwfmYCT5"
      },
      "outputs": [],
      "source": [
        "train_dataset = ds_train.take(20000)\n",
        "test_dataset = ds_train.skip(20000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0PrZBkGbaGk"
      },
      "source": [
        "## Generator\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7-P1PA4YHHt"
      },
      "source": [
        "The generator provides your training data. We create a mask by drawing random vertical lines at different angles and with varying widths. The portion of the image that is preserved is the part under the mask.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kRe6U2d6Kg4O"
      },
      "outputs": [],
      "source": [
        "def generator(dataset,nolines=9):\n",
        "    while True:  # Start an infinite loop\n",
        "        for batch in dataset:\n",
        "            images = batch[\"image\"]\n",
        "            images_np = images.numpy()\n",
        "\n",
        "            masks = np.zeros((batch_size, 64, 64))\n",
        "            for i in range(batch_size):\n",
        "                for j in range(nolines):\n",
        "                    start_point = (np.random.randint(0, 64 - 1), 0)\n",
        "                    end_point = (np.random.randint(0, 64 - 1), 63)\n",
        "                    thickness = np.random.randint(2, 3)\n",
        "                    masks[i] = cv2.line(masks[i], start_point, end_point, (1), thickness)\n",
        "\n",
        "            images_np = images_np / 255.0\n",
        "            masks = np.stack(((masks),) * 3, axis=-1)\n",
        "\n",
        "            yield (images_np * masks, images_np)\n",
        "\n",
        "# Batch the datasets\n",
        "batch_size = 100\n",
        "train_dataset_batched = train_dataset.batch(batch_size)\n",
        "test_dataset_batched = test_dataset.batch(batch_size)\n",
        "\n",
        "# Create generators for the batched datasets\n",
        "train_generator = generator(train_dataset_batched)\n",
        "test_generator = generator(test_dataset_batched)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fbvb4Kk5YNxP"
      },
      "source": [
        "Let's visualize the data. In the first row we show the damaged images, and in the second the originals that need to be reconstructed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 615
        },
        "id": "nvRhTSLrYNEf",
        "outputId": "07e606cb-8d84-4f78-824b-ae50a9e906bc"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(12, 8))  # Adjust figsize as needed\n",
        "a,b = next(train_generator)\n",
        "for i in range(3):\n",
        "  # Plot image on each subplot\n",
        "  axes[0,i].imshow(a[i])  # Use cmap='gray' if your images are grayscale\n",
        "  axes[0,i].axis('off')  # Turn off axis\n",
        "  axes[1,i].imshow(b[i])  # Use cmap='gray' if your images are grayscale\n",
        "  axes[1,i].axis('off')  # Turn off axis\n",
        "\n",
        "plt.tight_layout()  # Adjust subplots to fit into the figure area.\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j6BG_4YDX3ac"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from keras.layers import Input, Conv2D, Conv2DTranspose, MaxPooling2D, concatenate, Dropout, BatchNormalization\n",
        "from keras.models import Model\n",
        "from keras.callbacks import EarlyStopping, LearningRateScheduler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPfitdT7b2un"
      },
      "source": [
        "## U-net"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9wd8mpe-eUs_"
      },
      "outputs": [],
      "source": [
        "def down_block(inputs, filters, kernel_size=(3, 3), padding='same', activation='relu', dropout=0.5):\n",
        "  conv = Conv2D(filters, kernel_size, activation=activation, padding=padding)(inputs)\n",
        "  conv = BatchNormalization()(conv)\n",
        "  conv = Conv2D(filters, kernel_size, activation=activation, padding=padding)(conv)\n",
        "  pool = MaxPooling2D(pool_size=(2, 2))(conv)\n",
        "  pool = BatchNormalization()(pool)\n",
        "  pool = Dropout(dropout)(pool)\n",
        "  return conv, pool\n",
        "\n",
        "def up_block(inputs, skip, filters, kernel_size=(3, 3), padding='same', activation='relu',dropout=0.5):\n",
        "  up = Conv2DTranspose(filters, (2, 2), strides=(2, 2), padding='same')(inputs)\n",
        "  concat = concatenate([up, skip], axis=3)\n",
        "  conv = Dropout(dropout)(concat)\n",
        "  conv = Conv2D(filters, kernel_size=kernel_size, activation=activation, padding=padding)(conv)\n",
        "  conv = BatchNormalization()(conv)\n",
        "  conv = Conv2D(filters, kernel_size=kernel_size, activation=activation, padding=padding)(conv)\n",
        "  conv = BatchNormalization()(conv)\n",
        "  return conv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TznsYcJThhlI"
      },
      "outputs": [],
      "source": [
        "def nn():\n",
        "  inputs = Input((64, 64, 3))\n",
        "  # Down Blocks\n",
        "  conv1, pool1 = down_block(inputs, filters=64, dropout=0.25)\n",
        "  conv2, pool2 = down_block(pool1, filters=128)\n",
        "  conv3, pool3 = down_block(pool2, filters=256)\n",
        "\n",
        "  # bottleneck\n",
        "  conv4 = Conv2D(512, (3, 3), activation='relu', padding='same')(pool3)\n",
        "  conv4 = Conv2D(512, (3, 3), activation='relu', padding='same')(conv4)\n",
        "\n",
        "  # Up Blocks\n",
        "  conv5 = up_block(conv4, skip=conv3, filters=256)\n",
        "  conv6 = up_block(conv5, skip=conv2, filters=128)\n",
        "  conv7 = up_block(conv6, skip=conv1, filters=64)\n",
        "\n",
        "  conv8 = Conv2D(3, (3, 3), activation='sigmoid', padding='same')(conv7)\n",
        "  return Model(inputs=[inputs], outputs=[conv8])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WSLzIDY2Y4lm"
      },
      "source": [
        "## Hyperparameters\n",
        "\n",
        "- Batch size = 100\n",
        "- Max Epochs = 30\n",
        "- PATIENCE = 4\n",
        "- Initial Learning rate = 1e-3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K4ZmKAeBbKSE"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 30\n",
        "BATCH_SIZE = batch_size\n",
        "\n",
        "# Early stopping\n",
        "PATIENCE = 4\n",
        "\n",
        "# Learning rate\n",
        "LEARNING_RATE = 1e-3\n",
        "DECAY_FACTOR = 0.75\n",
        "STEP_LR = 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVyxUEvfk1ti"
      },
      "source": [
        "## Ottimizzatore e callbacks\n",
        "Per evitare l'overfitting del modello utilizzo l'Early Stopping: dopo PATIENCE epoche in cui non si registra un miglioramento sulla loss, l'allenamento termina.\n",
        "\n",
        "Per quanto riguarda la modifica del LearningRate durante il training, ho scelto di utilizzare lo Step Decay\n",
        "Schedule, ovvero il learning rate viene moltiplicato con un fattore 0*75 ogni 4 epoche. In tal modo si verrà a creare una discesa del learning rate a scalino. In questo modo è possibile ridurre il tempo di training e migliorare le performance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3U-I_KfefbQl"
      },
      "outputs": [],
      "source": [
        "EARLY_STOPPING = EarlyStopping(monitor='loss', mode='min', patience=PATIENCE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "OPTIMIZER = tf.keras.optimizers.legacy.Adam(learning_rate = LEARNING_RATE)\n",
        "\n",
        "def step_decay_schedule(initial_lr, decay_factor, step_size):\n",
        "    def schedule(epoch):\n",
        "        return initial_lr * (decay_factor ** np.floor(epoch/step_size))\n",
        "    return LearningRateScheduler(schedule)\n",
        "\n",
        "LR_SCHEDULE = step_decay_schedule(initial_lr=LEARNING_RATE, decay_factor=DECAY_FACTOR, step_size=STEP_LR)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNpjD5sDcQNs"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bWsVPF3Uk8P3"
      },
      "outputs": [],
      "source": [
        "model = nn()\n",
        "model.compile(optimizer=OPTIMIZER, loss='binary_crossentropy', metrics=['accuracy', 'mean_squared_error'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-D5fiApkavn"
      },
      "source": [
        "### Summary of the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.summary()\n",
        "keras.utils.plot_model(model, show_shapes=True, dpi=76)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "history = model.fit(train_generator, epochs=EPOCHS, steps_per_epoch=(20000 // BATCH_SIZE), callbacks = [EARLY_STOPPING, LR_SCHEDULE])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cUhYuQAl7H2"
      },
      "source": [
        "## Visualization of training history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 507
        },
        "id": "1SGIzl96jGYd",
        "outputId": "90f0fb7b-bdcb-4d72-ec10-a741a9c62801"
      },
      "outputs": [],
      "source": [
        "def plot_metrics(history):\n",
        "    fig, axes = plt.subplots(nrows=1, ncols=4, figsize=(15, 5))  # Imposta la dimensione della figura\n",
        "\n",
        "    axes[0].plot(history.history['loss'], color='blue')\n",
        "    axes[0].set_title(\"Binary CE\")\n",
        "    axes[0].set_xlabel(\"Epoca\")\n",
        "    axes[0].grid(True)\n",
        "\n",
        "    axes[1].plot(history.history['accuracy'], color='red')\n",
        "    axes[1].set_title(\"Accuracy\")\n",
        "    axes[1].set_xlabel(\"Epoch\")\n",
        "    axes[1].grid(True)\n",
        "\n",
        "    axes[2].plot(history.history['lr'], color='green')\n",
        "    axes[2].set_title(\"Learning rate\")\n",
        "    axes[2].set_xlabel(\"Epoch\")\n",
        "    axes[2].grid(True)\n",
        "\n",
        "    axes[3].plot(history.history['mean_squared_error'], color='green')\n",
        "    axes[3].set_title(\"Mean squared error\")\n",
        "    axes[3].set_xlabel(\"Epoch\")\n",
        "    axes[3].grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Utilizzo della funzione\n",
        "plot_metrics(history)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k9DIHF4Vk-gd"
      },
      "source": [
        "# Visualization of generated images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def showImages(x, y, z, customNcols=3, customFigSize=(12,8)):\n",
        "  # Adjust figsize as needed\n",
        "  fig, axes = plt.subplots(nrows=3, ncols=customNcols, figsize=customFigSize)\n",
        "\n",
        "  # Plot image on each subplot\n",
        "  for i in range(customNcols):\n",
        "    axes[0,i].imshow(x[i])  # Use cmap='gray' if your images are grayscale\n",
        "    axes[0,i].axis('off') # Turn off axis\n",
        "    axes[0,i].title.set_text(f\"img maschera {i}\")\n",
        "    axes[1,i].imshow(y[i])\n",
        "    axes[1,i].axis('off')\n",
        "    axes[1,i].title.set_text(f\"img ricostruita {i}\")\n",
        "    axes[2,i].imshow(z[i])\n",
        "    axes[2,i].axis('off')\n",
        "    axes[2,i].title.set_text(f\"img reale {i}\")\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "\n",
        "test_x, test_y = next(test_generator)\n",
        "inpainted_image = model.predict(test_x)\n",
        "showImages(test_x, inpainted_image, test_y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MMrZzKiPjW1j"
      },
      "source": [
        "## Evaluation of the model\n",
        "The mse is calculated on 10000 images generated from the test set for 10 times and the mean value and standard deviation are given."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "no_batch = 10000 // BATCH_SIZE\n",
        "mse_scores = []\n",
        "\n",
        "for i in range(10):\n",
        "  for j in range(no_batch):\n",
        "    mse = tf.keras.losses.MeanSquaredError()\n",
        "    test_x, test_y = next(test_generator)\n",
        "    prediction = model.predict(test_x, verbose=0)\n",
        "    mse_value = mse(test_y, prediction)\n",
        "    mse_scores.append(mse_value)\n",
        "\n",
        "mean_mse = np.mean(mse_scores)\n",
        "std_mse = np.std(mse_scores)\n",
        "\n",
        "print(f'Mean MSE: {mean_mse}, Std MSE: {std_mse}')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
