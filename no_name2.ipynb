{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mE7oyG0wv6e0"
      },
      "source": [
        "#Separation of CIFAR-10 Images\n",
        "\n",
        "The model takes as input an image created by averaging two random samples from CIFAR-10 and is tasked with predicting the categories of the two components.\n",
        "\n",
        "The first image belongs to the first five categories (airplane, automobile, bird, cat, deer), while the second belongs to the remaining categories (dog, frog, horse, ship, truck). The model must return two labels, each within a range of five possible values.\n",
        "\n",
        "As evaluation metric we use the mean classification accuracy for the two components.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iHjnh5XP0Sq4"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USdmzjiO0W6D"
      },
      "source": [
        "#Data Loading and preprocessing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yRYiW2ipukZF"
      },
      "outputs": [],
      "source": [
        "(cifar10_x_train, cifar10_y_train), (cifar10_x_test, cifar10_y_test) = cifar10.load_data()\n",
        "\n",
        "# Verify data shapes\n",
        "assert cifar10_x_train.shape == (50000, 32, 32, 3)\n",
        "assert cifar10_x_test.shape == (10000, 32, 32, 3)\n",
        "assert cifar10_y_train.shape == (50000, 1)\n",
        "assert cifar10_y_test.shape == (10000, 1)\n",
        "\n",
        "# Normalizing the images to the range [0, 1]\n",
        "cifar10_x_train = (cifar10_x_train / 255.).astype(np.float32)\n",
        "cifar10_x_test = (cifar10_x_test / 255.).astype(np.float32)\n",
        "\n",
        "classes = [\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZkiGnU4d0k4d"
      },
      "source": [
        "Let us split the images in two groups, according to their label."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dpey42Vo07Yb"
      },
      "outputs": [],
      "source": [
        "cond_1 = cifar10_y_train[:,0] < 5\n",
        "cifar10_x_train_1 = cifar10_x_train[cond_1]\n",
        "cifar10_y_train_1 = cifar10_y_train[cond_1]\n",
        "\n",
        "cond_2 = cifar10_y_train[:,0] >= 5\n",
        "cifar10_x_train_2 = cifar10_x_train[cond_2]\n",
        "cifar10_y_train_2 = cifar10_y_train[cond_2]\n",
        "\n",
        "cond_1_test = cifar10_y_test[:,0] < 5\n",
        "cifar10_x_test_1 = cifar10_x_test[cond_1_test]\n",
        "cifar10_y_test_1 = cifar10_y_test[cond_1_test]\n",
        "\n",
        "cond_2_test = cifar10_y_test[:,0] >= 5\n",
        "cifar10_x_test_2 = cifar10_x_test[cond_2_test]\n",
        "cifar10_y_test_2 = cifar10_y_test[cond_2_test]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "ey1LrhibPRbq",
        "outputId": "af3c68af-e59b-4ca3-c276-2c0c9ec8fcc2"
      },
      "outputs": [],
      "source": [
        "plt.imshow(cifar10_x_train_1[34])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmLYNuR-0s0m"
      },
      "source": [
        "Now we can define the generator. The input consists of two datasets (X1,X2), their corresponding labels (Y1,Y2), and a batch size.\n",
        "\n",
        "The generator returns (x_data,y_data), where:\n",
        "* x_data is a batch of images obtained by averaging random samples from X1 and X2.\n",
        "* y_data is a pair of batches of labels corresponding to the component images, expressed in categorical format.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Y5Zpv5fw2hd"
      },
      "outputs": [],
      "source": [
        "def datagenerator(X1,X2,Y1,Y2,batchsize):\n",
        "  size1 = X1.shape[0]\n",
        "  size2 = X2.shape[0]\n",
        "  Y1_cat = tf.keras.utils.to_categorical(Y1, num_classes=5)\n",
        "  Y2_cat = tf.keras.utils.to_categorical(Y2-5, num_classes=5)\n",
        "\n",
        "  while True:\n",
        "    num1 = np.random.randint(0, size1, batchsize)\n",
        "    num2 = np.random.randint(0, size2, batchsize)\n",
        "    x_data = (X1[num1] + X2[num2]) / 2.0\n",
        "    y_data = (tf.convert_to_tensor(Y1_cat[num1]), tf.convert_to_tensor(Y2_cat[num2]))\n",
        "\n",
        "    yield x_data, y_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9lf3TuP2pdQ"
      },
      "source": [
        "Let us instantiate a generator on Cifar10 with batchsize=1, and let's check its behaviour."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "29TldJ6-720b"
      },
      "outputs": [],
      "source": [
        "datagen = datagenerator(cifar10_x_train_1,cifar10_x_train_2,cifar10_y_train_1,cifar10_y_train_2,1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1DrJVzI3ysV"
      },
      "source": [
        "Let's generate an example, display the image that the model will take as input, and print the categories of the two overlapping components.\n",
        "\n",
        "You can re-run the cell to display new examples.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 465
        },
        "collapsed": true,
        "id": "qL1sMtjG8VmG",
        "outputId": "914604f2-2b89-4bd9-a776-928beceb27ce"
      },
      "outputs": [],
      "source": [
        "x, y = next(datagen)\n",
        "\n",
        "print(\"first: {}, second = {}\".format(classes[np.argmax(y[0][0])],classes[np.argmax(y[1][0])+5]))\n",
        "plt.imshow(x[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5fcOMOQhfI8"
      },
      "source": [
        "## Model\n",
        "\n",
        "We use the AdamW optimizer with categorical_crossentropy as loss function. During training, we adopt `EarlyStopping` to avoid overfitting and `ReduceLROnPlateau` to dynamically adapt the learning rate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_SoRCZZyhnzb"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "BATCH_SIZE = 256\n",
        "\n",
        "optimizer = optimizers.AdamW(learning_rate=0.001)\n",
        "loss={\"out1\": \"categorical_crossentropy\", \"out2\": \"categorical_crossentropy\"}\n",
        "metrics=[\"accuracy\", \"accuracy\"]\n",
        "\n",
        "X_train_1, X_val_1, Y_train_1, Y_val_1 = train_test_split(\n",
        "    cifar10_x_train_1, cifar10_y_train_1, test_size=0.1, random_state=42\n",
        ")\n",
        "X_train_2, X_val_2, Y_train_2, Y_val_2 = train_test_split(\n",
        "    cifar10_x_train_2, cifar10_y_train_2, test_size=0.1, random_state=42\n",
        ")\n",
        "\n",
        "# Generatori con i nuovi set di dati\n",
        "train_generator = datagenerator(X_train_1, X_train_2, Y_train_1, Y_train_2, BATCH_SIZE)\n",
        "val_generator = datagenerator(X_val_1, X_val_2, Y_val_1, Y_val_2, BATCH_SIZE)\n",
        "\n",
        "\n",
        "# Callbacks\n",
        "early_stopping = EarlyStopping(monitor=\"val_loss\", patience=10, restore_best_weights=True)\n",
        "reduce_lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.7, patience=3, min_lr=1e-4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9CZ6ij1NjJna"
      },
      "source": [
        "## Residual Block\n",
        "`conv_block` is a residual block, composed by two/three convolutional layers, usually intermixed with BatchNomralization layers.\n",
        "\n",
        "\n",
        "Il pooling opzionale consente di ridurre la dimensionalità spaziale, mentre il dropout aggiunge regolarizzazione per prevenire l’overfitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2G2YrP2-jUNT"
      },
      "outputs": [],
      "source": [
        "def conv_block(filters, x, pooling=True):\n",
        "    shortcut = x\n",
        "\n",
        "    x = layers.Conv2D( filters, kernel_size=3, padding=\"same\")(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.LeakyReLU(alpha=0.1)(x)\n",
        "\n",
        "    x = layers.Conv2D( filters, kernel_size=3, padding=\"same\")(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.LeakyReLU(alpha=0.1)(x)\n",
        "\n",
        "    x = layers.Conv2D( filters, kernel_size=1, padding=\"same\")(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "\n",
        "    if shortcut.shape[-1] != filters:\n",
        "        shortcut = layers.Conv2D( filters, kernel_size=1, padding=\"same\")(shortcut)\n",
        "\n",
        "    x = layers.Add()([x, shortcut])\n",
        "    x = layers.LeakyReLU(alpha=0.1)(x)\n",
        "\n",
        "    # optinal Pooling\n",
        "    if pooling:\n",
        "        x = layers.MaxPooling2D(pool_size=2, padding=\"same\")(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Dropout(0.3)(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXiKAAofBCCj"
      },
      "source": [
        "## The model\n",
        "The `my_net` function defines a CNN designed to work with images of size 32x32x3.\n",
        "\n",
        "The network uses a series of convolutional blocks to extract features from the data, alternating convolutions with and without pooling to maintain a good balance between detail and dimensionality reduction.\n",
        "\n",
        "After feature extraction, the network branches into two dense outputs, each with 5 neurons and a softmax activation, suitable for multiclass classification on the two groups in question in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cIW5Zk3UjMBE"
      },
      "outputs": [],
      "source": [
        "def my_net(input_shape=(32, 32, 3)):\n",
        "    inputs = layers.Input(input_shape)\n",
        "\n",
        "    x = inputs\n",
        "    x = conv_block(32, x, pooling=False)\n",
        "    x = conv_block(64, x)\n",
        "    x = conv_block(128, x, pooling=False)\n",
        "    x = conv_block(196, x)\n",
        "    x = conv_block(256, x)\n",
        "    x = layers.Flatten()(x)\n",
        "\n",
        "    #branching\n",
        "    output1 = layers.Dense(5, activation=\"softmax\", name=\"out1\")(x)\n",
        "    output2 = layers.Dense(5, activation=\"softmax\", name=\"out2\")(x)\n",
        "\n",
        "    return Model(inputs, [output1, output2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adPbcUarWImk"
      },
      "source": [
        "Let us compile the network and show the summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "CAKqYlzlkEHu",
        "outputId": "ea5b9575-c8ca-4f5d-f137-a185205d36bc"
      },
      "outputs": [],
      "source": [
        "model = my_net()\n",
        "model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2wkW9sfukXnM"
      },
      "source": [
        "Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "vfGvrZ-GkZjo",
        "outputId": "3934743f-10a1-4c05-d91e-67f97628fe20"
      },
      "outputs": [],
      "source": [
        "history = model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=1300,\n",
        "    epochs=100,\n",
        "    validation_data=val_generator,\n",
        "    validation_steps=300,\n",
        "    callbacks=[reduce_lr, early_stopping],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9gBurS7ilZS"
      },
      "source": [
        "Training info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 599
        },
        "id": "LGXU32zNi4WZ",
        "outputId": "68310236-ba62-4894-ab33-01aed61dec81"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.style.use(\"dark_background\")\n",
        "\n",
        "fig, (grafico1, grafico2, grafico3) = plt.subplots(1, 3, figsize=(18, 6))\n",
        "\n",
        "colori = {\n",
        "    \"out1_loss\": \"#e74c3c\", \"val_out1_loss\": \"#ffb3b3\",\n",
        "    \"out2_loss\": \"#3498db\", \"val_out2_loss\": \"#85c1e9\",\n",
        "    \"out1_accuracy\": \"#27ae60\", \"val_out1_accuracy\": \"#a9dfbf\",\n",
        "    \"out2_accuracy\": \"#9b59b6\", \"val_out2_accuracy\": \"#d7bde2\",\n",
        "    \"learning_rate\": \"#f1c40f\"\n",
        "}\n",
        "\n",
        "# Loss\n",
        "grafico1.plot(history[\"out1_loss\"], label=\"Train Loss (out1)\", color=colori[\"out1_loss\"], linewidth=2)\n",
        "grafico1.plot(history[\"val_out1_loss\"], label=\"Val Loss (out1)\", color=colori[\"val_out1_loss\"], linestyle=\"--\", linewidth=2)\n",
        "grafico1.plot(history[\"out2_loss\"], label=\"Train Loss (out2)\", color=colori[\"out2_loss\"], linewidth=2)\n",
        "grafico1.plot(history[\"val_out2_loss\"], label=\"Val Loss (out2)\", color=colori[\"val_out2_loss\"], linestyle=\"--\", linewidth=2)\n",
        "grafico1.set_title(\"Loss history\", fontsize=16)\n",
        "grafico1.set_xlabel(\"Epochs\", fontsize=12)\n",
        "grafico1.set_ylabel(\"Loss\", fontsize=12)\n",
        "grafico1.legend(fontsize=10)\n",
        "grafico1.grid(True)\n",
        "\n",
        "# Accuracy\n",
        "grafico2.plot(history[\"out1_accuracy\"], label=\"Train Accuracy (out1)\", color=colori[\"out1_accuracy\"], linewidth=2)\n",
        "grafico2.plot(history[\"val_out1_accuracy\"], label=\"Val Accuracy (out1)\", color=colori[\"val_out1_accuracy\"], linestyle=\"--\", linewidth=2)\n",
        "grafico2.plot(history[\"out2_accuracy\"], label=\"Train Accuracy (out2)\", color=colori[\"out2_accuracy\"], linewidth=2)\n",
        "grafico2.plot(history[\"val_out2_accuracy\"], label=\"Val Accuracy (out2)\", color=colori[\"val_out2_accuracy\"], linestyle=\"--\", linewidth=2)\n",
        "grafico2.set_title(\"Accuracy history\", fontsize=16)\n",
        "grafico2.set_xlabel(\"Epoche\", fontsize=12)\n",
        "grafico2.set_ylabel(\"Accuracy\", fontsize=12)\n",
        "grafico2.legend(fontsize=10)\n",
        "grafico2.grid(True)\n",
        "\n",
        "# Learning Rate\n",
        "grafico3.plot(history.history[\"learning_rate\"], label=\"Learning Rate\", color=colori[\"learning_rate\"], linewidth=2)\n",
        "grafico3.set_title(\"Learning Rate history\", fontsize=16)\n",
        "grafico3.set_xlabel(\"Epoche\", fontsize=12)\n",
        "grafico3.set_ylabel(\"Learning Rate\", fontsize=12)\n",
        "grafico3.legend(fontsize=10)\n",
        "grafico3.grid(True)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5lzBotwL5QN"
      },
      "source": [
        "# Evalaution\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_p4UuG1QF8t"
      },
      "source": [
        "Now let's use the test generator and proceed with the evaluation of my model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQo8_6w-L4WY",
        "outputId": "961e012e-2fac-4383-e8ab-426a10d26dbb"
      },
      "outputs": [],
      "source": [
        "testgen = datagenerator(cifar10_x_test_1,cifar10_x_test_2,cifar10_y_test_1,cifar10_y_test_2,10000)\n",
        "\n",
        "eval_samples_x, eval_samples_y = next(testgen)\n",
        "print(eval_samples_x.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gomFTuuDOy8A"
      },
      "outputs": [],
      "source": [
        "def eval_model(model):\n",
        "    eval_samples_x, eval_samples_y = next(testgen)\n",
        "    predictions = model.predict(eval_samples_x)\n",
        "    predictions_list = np.column_stack([np.argmax(predictions[0], axis=1), np.argmax(predictions[1], axis=1)])\n",
        "    correct_guesses_1 = predictions_list[:, 0] == np.argmax(eval_samples_y[0], axis=1)\n",
        "    correct_guesses_2 = predictions_list[:, 1] == np.argmax(eval_samples_y[1], axis=1)\n",
        "    return (np.mean(correct_guesses_1) + np.mean(correct_guesses_2)) / 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7usBI88dje70"
      },
      "source": [
        "Let's repeat the evaluation on my model 10 times so that we can analyze its standard deviation of the measured accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZFu8iEt9jdZA",
        "outputId": "4c2fbfdd-106f-4bfb-d993-396e8c5c2ad2"
      },
      "outputs": [],
      "source": [
        "repeat_eval = 10\n",
        "eval_results = []\n",
        "\n",
        "for i in range(repeat_eval):\n",
        "    em = eval_model(model)\n",
        "    print(\"accuracy\", i+1, \"=\",em)\n",
        "    eval_results.append(em)\n",
        "\n",
        "# Risultati finali\n",
        "print(\"------------------------------------------\")\n",
        "print(\"mean accuracy = \", np.mean(eval_results))\n",
        "print(\"standard deviation = \", np.std(eval_results))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
