{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qTY7Bq7hKwYN"
      },
      "source": [
        "#Metric Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPyuecNNLL-M"
      },
      "source": [
        "##When two image are similar ?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import imageio.v2 as imageio\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "\n",
        "codes=[ '1igH62U1vMRrajeVAegZ5-3gsINmA-5x1 ', '10Q3CYRW4hpcuuZec5eOuXvwQ8OD8OZ7j','11BFlXWR5PfGYYlmTOJ0l_Cz3doDWtd8K' ]\n",
        "img_shape = (450,300,3)\n",
        "\n",
        "# codes=['1f-tn7K_q2Qvtxeh3C27MQPIh6uEzoVQZ','1e448rCVsEJ1jGsk2MyoReeecIHCWWkMe', \"1GSWnue00CcWETZ0_5vEGY7nCIDNWIEad\",\"106hzZt6Vz49DXuKPt-K-ZzoKgR4_GZ4m\"]\n",
        "# img_shape = (224,224,3)\n",
        "\n",
        "\n",
        "images=[]\n",
        "for c in codes :\n",
        "  url=\"https://drive.google.com/uc?id=\"\n",
        "  img = imageio.imread(url+c)\n",
        "  img = cv2.resize(img, (img_shape[0],img_shape[1]))\n",
        "  images.append(img)\n",
        "\n",
        "\n",
        "def plot_images():\n",
        "  fig, axes = plt.subplots(1, len(images), figsize=(15, 5))\n",
        "  for idx, (ax, img) in enumerate( zip(axes, images) ):\n",
        "      ax.set_title(f'#{idx}')\n",
        "      ax.imshow(img)\n",
        "      ax.axis('off')\n",
        "\n",
        "  plt.show()\n",
        "\n",
        "plot_images()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import pairwise_distances\n",
        "\n",
        "def norm_euclid( x, eps=1e-10 ):\n",
        "  dist =np.sqrt(((x[:, np.newaxis] - x) ** 2).sum(axis=2))\n",
        "  norm_dist = dist / (np.max(dist) + eps)\n",
        "  return norm_dist\n",
        "\n",
        "\n",
        "imgs = np.array(images) //255\n",
        "x = np.array([xx.flatten() for xx in imgs])\n",
        "\n",
        "dists=norm_euclid(x) #error at index 1\n",
        "np.fill_diagonal(dists, 1.0 )\n",
        "similarities = np.argmin(dists, axis=1)\n",
        "\n",
        "print( \"Distance Matrix:\\n\",dists )\n",
        "print( \"\\nMost similar index:\\n\",similarities )\n",
        "plot_images()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXblvf9GGwy-"
      },
      "source": [
        "Problem: How can we cluster images according to their content?\n",
        "Background can influence the classification. If we want to base the similarity on the image content, we need somenting more powerfull."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fs2msQpIktKN"
      },
      "source": [
        "**Embeddings can solve our problem.**\n",
        "\n",
        "The numerical vector representation of images, words, senteces, or even music is commonly called **embeddings** and the multidimensinal space where embeddings lies is called **embedding space.**\n",
        "\n",
        "In this space, the distance or direction of the feature vectors reflects the semantic relations between two entities.\n",
        "\n",
        "For the purpose of our example, we want to compute embeddings that represent the visual content of the image.\n",
        "\n",
        "Actually when we build a classifier, we train the network to identify the content of the images.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4x_Q3MwKzSx"
      },
      "source": [
        "What if we want to do the same thing with another class ?\n",
        "\n",
        "Do we need to train again the model?\n",
        "And what if we lack sufficient images for training and testing?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alw0oL4_lZ19"
      },
      "source": [
        "We can see he problem as\n",
        "**Let's use transfert learning**:\n",
        "we can apply ResNet50 as backbone by removing its last layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from keras.applications import ResNet50V2\n",
        "from keras.models import Model\n",
        "\n",
        "\n",
        "h = img_shape[1]\n",
        "w = img_shape[0]\n",
        "c = img_shape[2]\n",
        "\n",
        "feat_extractor = ResNet50V2(weights='imagenet', include_top=False, pooling=\"avg\",  input_shape=(h,w,c))\n",
        "\n",
        "print( \"embedding size:\",feat_extractor.output.shape)\n",
        "\n",
        "model = Model(inputs=feat_extractor.input, outputs=feat_extractor.output)\n",
        "#compute embeddigns\n",
        "embs = model( imgs ).numpy()\n",
        "dists = norm_euclid( embs )\n",
        "np.fill_diagonal(dists, 1.0 )\n",
        "similarities = np.argmin( dists, axis=-1)\n",
        "\n",
        "\n",
        "print( \"Distance Matrix:\\n\",dists )\n",
        "print( \"Similarities (minimum):\\n\",similarities )\n",
        "plot_images()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lRzJvMbNA1s6"
      },
      "source": [
        "what if we change images ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t_WA_nGqENyk"
      },
      "source": [
        "##Embedding Space Organization\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "id": "Xl0xeefJ-w9t",
        "outputId": "a9541aa2-2d66-46ad-e266-4a7111c6f626"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# norm_embs = np.sqrt(np.sum(embs**2, axis=1, keepdims=True))  #l2 normalization\n",
        "\n",
        "dist_mat = cosine_similarity(embs,embs)\n",
        "np.fill_diagonal(dist_mat, 0)\n",
        "\n",
        "top_similar = np.argmax( dist_mat, axis=1)\n",
        "\n",
        "print( \"Distance Matrix:\\n\",dist_mat )\n",
        "print( \"Similar (max):\\n\",top_similar )\n",
        "plot_images()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E31e6YKOFRnD"
      },
      "source": [
        "When we change distribution the background can still influence the classification. Now immagine if we open the problem towards a new class. We should retrain the network every time.\n",
        "\n",
        "So what is a good embedding ? which caractheristic the embedding should have ?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "tX7tzWDS3nVc",
        "outputId": "ece25e14-babe-40d2-f20c-fd98a5487a4b"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import make_blobs\n",
        "X, y = make_blobs(n_samples=1000, centers=5, random_state=42)\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis')\n",
        "plt.title('Blobs')\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z54XDLmGNhTd"
      },
      "source": [
        "##Change now the perspective of the problem as verification problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqWoyLvjOJ59"
      },
      "source": [
        "If two images depitch the same \"instance\" then they are similar and they have a similar embedding. Otherwise the two immages are different.\n",
        "\n",
        "If the embedding space is large enough for deptching a new class and I have enough training data, even for different instances, the network would be able to generalize better for the new examples.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HcL2DF1tQWxN"
      },
      "source": [
        "##Metric Learning\n",
        "\n",
        "The main idea of metric learning is of using a loss at training time for drive the feature extractor network towards a better and more discriminative reppresentation of embeddings where the intra-class distance (similarity distances between same classes) is minimize and at the same time the inter-class distance (similarity distances between different classes) is maximized.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mQbtnhxbOJVZ"
      },
      "outputs": [],
      "source": [
        "#Contrastive loss\n",
        "#Used to push far away or closer the embeddings\n",
        "def contrastive_loss(embs1,embs2, label1, label2, dist_fun, m=0.1 ):\n",
        "\n",
        "  same = K.cast(label1==label2, dtype=tf.float32)\n",
        "  dist= dist_fun(embs1,embs2)\n",
        "\n",
        "  loss = same*(dist+m) + (1-same) * K.max(0,dist+m)\n",
        "\n",
        "  return K.mean(loss, axis=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFfaOGvmU2jr"
      },
      "source": [
        "The margin **m** is important:\n",
        "\n",
        "*   Positive cases: marging prevents that embeddings converge to the same point\n",
        "*   Negative cases: avoid pushing different embeddings far. In the practice it does not take any practival advantages.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AkdLn7McOJSr"
      },
      "outputs": [],
      "source": [
        "#Triplet Loss\n",
        "#A generalization of the problem from verification to triplets (ancor, positive, negatives)\n",
        "# Formula is dist_ap + α < dist_an\n",
        "def triplet_loss( anchors, positives, negatives, dist_fun, margin=0.1):\n",
        "\n",
        "  dist_ap = dist_fun(anchors, positives)\n",
        "  dist_an = dist_fun(anchors, negatives)\n",
        "\n",
        "  loss = K.maximum(0.0, (dist_ap-dist_an)+margin)\n",
        "  return K.mean(loss, axis=0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbJoab52XESV"
      },
      "source": [
        "The **semi-hard triplet sampling strategy** was proven to be effective.\n",
        "First sample C*K images from the dataset. Then,pick the hardest positive (very different) and the hardest negative (very similar) of the batch.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wk46Tp8UcaUe"
      },
      "source": [
        "Another version of triplet loss is done in this way\n",
        "\n",
        "\n",
        "```\n",
        "y = K.ones(shape=K.shape(dist_an))\n",
        "loss = K.max( 0.0, -y* ( dist_ap - dist_an)+margin)\n",
        "return K.mean(loss, axis=0)\n",
        "\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jCOPEpRtiob3"
      },
      "source": [
        "##Modern Embedding shaping\n",
        "\n",
        "\n",
        "*   Classification with softmax becomes a better pre-training objective, but clusters are not well separated.\n",
        "\n",
        "*   During training, the output embedding is normalized by dividing it for its l2 norm, so the values are inside an hypersphere.\n",
        "\n",
        "*   As a consequence we can use an angular similarity function like cosine similarity function for computing the similarity among embeddings.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wEkRv3PEEYQU"
      },
      "source": [
        "ArcFace loss introduce un margine angolare additivo nella funzione di perdita softmax. Il margine angolare additivo è un parametro che controlla la separabilità tra le classi. Un valore di margine più alto porta a una maggiore separabilità tra le classi.\n",
        "\n",
        "https://arxiv.org/pdf/1801.07698"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.initializers import glorot_uniform\n",
        "\n",
        "\n",
        "#ArcFace Loss\n",
        "class ArcFace(tf.keras.losses.Loss):\n",
        "\n",
        "  def __init__(self, num_classes, emb_dim, margin=0.5, scale=0.64, eps=K.epsilon() ):\n",
        "    super().__init__()\n",
        "    self.weights = tf.Variable(glorot_uniform(shape=(num_classes, emb_dim)), dtype=tf.float32)\n",
        "    self.margin = margin\n",
        "    self.scale = scale\n",
        "    self.eps = eps\n",
        "\n",
        "\n",
        "  def call(self, embs, labels):\n",
        "    # Normalize feature vectors to compare\n",
        "    embs_norm = tf.nn.l2_normalize(embs, axis=1)\n",
        "    norm_weight = tf.nn.l2_normalize( self.weights, axis=0)\n",
        "    w_label = tf.gather(norm_weight, label, axis=1)\n",
        "\n",
        "    #Cosine similarity between features and weights\n",
        "    cosine_sim = tf.matmul(embs_norm, tf.transpose(w_label) )\n",
        "    cosine_sim = tf.clip_by_value(cosine_sim, -1.0+eps, 1.0-eps)\n",
        "\n",
        "    theta = tf.acos(cosine_sim)\n",
        "    probs = s * tf.math.cos(theta + m) #multiply by s project into the axis space of the sphere\n",
        "\n",
        "    # One-hot encode the labels\n",
        "    one_hot = tf.one_hot(tf.cast(label, tf.int32), depth=num_classes) #may have the label smoothing\n",
        "    loss = tf.nn.softmax_cross_entropy_with_logits(labels=one_hot, logits=probs )\n",
        "\n",
        "    return tf.reduce_mean(loss)\n",
        "\n",
        "##eventually think about using label smoothing on cross entropy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNflALitZrci"
      },
      "source": [
        "We can obtain a loss considering all negatives in a batch by computing the cross-entropy loss of the softmax of the cosine similarities between the anchor and all the other samples, where the positive sample acts as the positive class.\n",
        "\n",
        "To make the loss focus on a larger set of negatives we can reduce the differences between the cosine similarities using a temperature τ. In this case we obtained the so-called normalized temperature-scaled cross entropy loss. It is the NT-Xent loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Tz14nR9OI_W"
      },
      "outputs": [],
      "source": [
        "#Xent loss\n",
        "def nt_xent_loss(tau):\n",
        "  #tau is the value of the temperature, it can be a scheduler\n",
        "\n",
        "  def loss(label, embs ):\n",
        "    # Normalize embeddings\n",
        "    embs = tf.math.l2_normalize(embs, axis=1)\n",
        "\n",
        "    # Compute cosine similarity matrix\n",
        "    cosine_sim = tf.matmul(embs, embs, transpose_b=True)\n",
        "\n",
        "    # Exclude diagonal from similarity matrix (similarity of each embedding with itself)\n",
        "    mask = tf.eye(cosine_sim.shape[0]) * -np.inf\n",
        "    cosine_sim = cosine_sim + mask\n",
        "\n",
        "    # Scale cosine similarities by temperature\n",
        "    #By dividing by temperature τ the cosine similarities we can focus on a larger set.\n",
        "    cosine_sim = cosine_sim / tau\n",
        "\n",
        "    # Compute cross-entropy loss using TensorFlow's built-in function\n",
        "    loss = tf.nn.softmax_cross_entropy_with_logits(logits=cosine_sim, labels=labels)\n",
        "    return tf.reduce_mean(loss)\n",
        "\n",
        "  return loss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gepws6wsoD1p"
      },
      "source": [
        "In a batch all the samples are from different classes.\n",
        "Paper: https://papers.nips.cc/paper_files/paper/2016/hash/6b180037abbebea991d8b1232f8a8ca9-Abstract.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0GVH-Qpqo5gZ"
      },
      "source": [
        "##Connecting the dots\n",
        "\n",
        "\n",
        "\n",
        "*   Metric Learning and Cross entropy can be trained jointly\n",
        "*   Such techniques can be used in several open world problems:\n",
        "\n",
        "\n",
        "    *   We don't know how to actually quantitatively measure the identity preservation.\n",
        "    *   Few shot learning\n",
        "    *   Re-identification\n",
        "    *   Fine-grained content-based image retrieval (CBIR)\n",
        "    \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ieE9zLWTrZ_M"
      },
      "source": [
        "##Clip\n",
        "this work has more than 16K citations\n",
        "https://arxiv.org/abs/2103.00020\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "woSY0Krkui4A"
      },
      "outputs": [],
      "source": [
        "# Numpy-like pseudocode for the core of an implementation of CLIP\n",
        "\n",
        "\n",
        "# image_encoder - ResNet or Vision Transformer\n",
        "# text_encoder - CBOW or Text Transformer\n",
        "# I[n, h, w, c] - minibatch of aligned images\n",
        "# T[n, l] - minibatch of aligned texts\n",
        "# W_i[d_i, d_e] - learned proj of image to embed\n",
        "# W_t[d_t, d_e] - learned proj of text to embed\n",
        "# t - learned temperature parameter\n",
        "\n",
        "# extract feature representations of each modality\n",
        "I_f = image_encoder(I) #[n, d_i]\n",
        "T_f = text_encoder(T) #[n, d_t]\n",
        "\n",
        "# joint multimodal embedding [n, d_e]\n",
        "I_e = l2_normalize(np.dot(I_f, W_i), axis=1)\n",
        "T_e = l2_normalize(np.dot(T_f, W_t), axis=1)\n",
        "\n",
        "# scaled pairwise cosine similarities [n, n]\n",
        "logits = np.dot(I_e, T_e.T) * np.exp(t)\n",
        "\n",
        "# symmetric loss function\n",
        "labels = np.arange(n)\n",
        "loss_i = cross_entropy_loss(logits, labels, axis=0)\n",
        "loss_t = cross_entropy_loss(logits, labels, axis=1)\n",
        "loss = (loss_i + loss_t)/2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import TFCLIPModel, CLIPProcessor\n",
        "\n",
        "model = TFCLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "id": "mSTSSV9xu9fG",
        "outputId": "209104bc-a36f-4196-d3ea-3e7228304d97"
      },
      "outputs": [],
      "source": [
        " ##let's try again with the photo of before\n",
        "fig, axes = plt.subplots(1, len(images), figsize=(15, 5))\n",
        "for idx, (ax, img) in enumerate( zip(axes, images) ):\n",
        "    ax.set_title(f'#{idx}')\n",
        "    ax.imshow(img)\n",
        "    ax.axis('off')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lr2l1A2Tu9cY",
        "outputId": "5ff022fd-0819-411d-fb0a-94435fdd7a17"
      },
      "outputs": [],
      "source": [
        "\n",
        "text_queries = [\"A photo of a white cat\",\"A photo of a brown cat\",\"A photo of a dog\"]\n",
        "gallery_images = np.array(images)\n",
        "inputs = processor( text=text_queries, images=gallery_images,padding=True, return_tensors=\"tf\")\n",
        "outputs = model(**inputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RA9ghsJ_u9V_",
        "outputId": "c15fee1c-9ae2-4080-c848-05767a6d679d"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "logits_per_text = outputs.logits_per_text  # this is the text-image similarity score\n",
        "probs = tf.nn.softmax(logits_per_text, axis=1)\n",
        "\n",
        "\n",
        "for i,q in enumerate( text_queries):\n",
        "  print(\"\\n\")\n",
        "  print(f\"Query: '{q}'\")\n",
        "  print(f\"Probability scores: { np.round( probs.numpy()[i],4) }\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
