{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cb50451",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Lecture 4 - Backpropagation\n",
    "In this lecture:\n",
    "\n",
    "1) A review of the backpropagation process\n",
    "2) Numpy implementation of a dense neural \n",
    "3) Tensorflow and Keras basics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72fdb56b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## A brief review of backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3829d173",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "Each neuron is defined by a bias $b$ a set of inputs $X_i$ and an associated weight $w_i$ for each input. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1aeeb1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "A single perceptron outputs  $ out = \\sigma(z)$ where $$z = \\Sigma{w_i x_i} + b$$\n",
    "\n",
    "We call $z$ the __weighted output__ of the neuron and $\\sigma$ the __activation function__. The activation function introduces non-linearity to the model, allowing the neuron to learn complex patterns and relationships in the data. In our example we use the sigmoid function : $\\sigma(x) =  \\frac{e^x}{1 + e^x}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2b415a-3d0d-4936-9f9a-12b8a2046e25",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "*why we need non linearity in neural networks? Because information is full of non linear relationships, that we need to effectively model*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4468fe51",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "If we dispose the perceptrons in layers, we can easily __parallelize these computations using matrix operations__. We address the parallelized quantities by an apex specifying the layer. For example $z^l$ means all the weighted outputs of the layer $l$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd35ef0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "To assess the quality of our output we need to define a cost function. The cost function depends only on the current output. Intuitively, the loss function provides the feedback that we need to propagate trough the network to improve the overall performance. We call the cost function $C$ and in our example we use the MSE cost function. $MSE = \\frac{1}{n} \\sum_n (y_i - \\hat{y}_i)^2$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d2f061",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "To better understand the backpropagation algorithm we define some supporting quantities. We define the __activation__ $a^l$ for the layer l as:  \n",
    "$$a^l = \\sigma(z^l)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe32863-fe5b-47b6-b983-9c5264b9c2d9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "We can thus define the __weighted output__ $z^l$ for a given layer as a function of weights, biases and activations of the previous layers :\n",
    "$$ z^l = w^l a^{l-1} + b^l$$\n",
    "Finally we introduce the __error__ $\\delta$, which for a specific neuron $j$ in layer $l$ is defined as:\n",
    "$$\\delta_j^{l} = \\frac{\\partial C}{\\partial z_j^{l}}$$\n",
    "which can be interpreted as the variation on the cost function caused by the weighted output of a neuron."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ceb125",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Now lets take a look at the actual backpropagation algorithm, the proof for the backpropagation equations are all obtainable via the chain rule.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0d4617-09c6-468f-bd5d-91e0b038f2ec",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "where $\\odot$ is the hadamard product, and $\\nabla_a$ is the derivative of the cost function with respect of the activation vector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03474b73",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "Do you remember the chain rule?\n",
    "\n",
    "$$ \\frac{dy}{dx} = \\frac{dy}{du} \\cdot \\frac{du}{dx} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1284cf",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "#### the algorithm can be summarized in :\n",
    "1) Given the input calculate all the weighted outputs and activations for all the layers\n",
    "2) Compute the error for the last layer \n",
    "3) Propagate backward by calculating the errors layer by layer\n",
    "4) Finally calculate the variation of the cost with respect of bias and weights \n",
    "5) Use an optimization algorithm such as gradient descent to update bias end weights "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7269a1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "A nice resource to better understand backpropagation and neural networks in general [Neural Networks and deep learning book](http://neuralnetworksanddeeplearning.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e66ef4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Implementing the algorithm, using only numpy\n",
    "Testing our neural network with a classic classification task on a variation of the Mnist dataset called KMnist.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f11eef",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython.display import HTML\n",
    "\n",
    "#import loader\n",
    "import keras\n",
    "import time \n",
    "import copy\n",
    "# Util for pwd \n",
    "import gzip\n",
    "cwd = os.getcwd()\n",
    "# Data needs to be normalized \n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576b1e81-03c7-4cfa-98f6-40960019517f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311786c3-1ba6-40d7-861a-b0f2b7b3e943",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### numpy basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd72274-5622-42dd-8979-256a7b6e4b8f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0572370-28c2-40b0-b7e4-2263610a6af0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_train.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b996db9b-6fbe-4769-8763-21cd27fe375e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_train.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1032b6-dcea-493d-8216-30cce0814a10",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Normalization \n",
    "x_train = x_train / 255 \n",
    "x_test = x_test / 255 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba6eee3-0567-47c5-89cb-14d7124f1d1c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.imshow(x_train[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a60359-a682-4833-8327-dac825fe4437",
   "metadata": {},
   "outputs": [],
   "source": [
    "slicing = x_train[:,-14:,14:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0187c5a1-7f64-42dd-a05b-cf48ad0dc868",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(slicing[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d8f9d7-eb41-4af8-bdea-5ad3cf8da51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "slicing = x_train[:,::-1,::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a31209e-f093-4879-9633-c65201d97818",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(slicing[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3876c0a8-e6fb-4e61-aacd-0b2141fceb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(x):\n",
    "    if(x==0):\n",
    "        return np.random.rand() \n",
    "    else:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef38eaf6-54da-4d76-a622-f23c48a7407d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vfunc = np.vectorize(func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b452c23d-4005-46dc-a222-6b34fbb8e2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = vfunc(x_train[0:10])\n",
    "plt.imshow(test[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e303c4-40a2-4e1d-84b7-aeda59b9e57f",
   "metadata": {},
   "source": [
    "### Going back at neural networks..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8a7781",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#Given a dimension, create a random vector of biases\n",
    "def set_biases(bias_matrix_dim,rnd):\n",
    "    tmp = []\n",
    "    for i in bias_matrix_dim:\n",
    "        tmp.append(rnd.randn(i, 1))\n",
    "    return tmp\n",
    "\n",
    "def prepare_weightMatrix_dim(sizes):\n",
    "    tmp = []\n",
    "    for i in range(len(sizes)-1):\n",
    "        tmp.append([sizes[i],sizes[i+1]])\n",
    "    return tmp    \n",
    "\n",
    "#Given a list of tuples, create random vector of matrices\n",
    "def set_weights(weight_matrix_dim,rnd):\n",
    "    tmp = []\n",
    "    for i in weight_matrix_dim:\n",
    "        tmp.append(rnd.randn(i[1], i[0]))\n",
    "    return tmp\n",
    "\n",
    "\n",
    "def split_data(training_data,mini_batch_size): \n",
    "    tmp = []\n",
    "    for i in range(0,len(training_data),mini_batch_size):\n",
    "        tmp.append(training_data[i:i+mini_batch_size])\n",
    "    return tmp\n",
    "\n",
    "#Produce the one hot encoding for class labels\n",
    "def unit_vector(j):\n",
    "    e = np.zeros((10, 1))\n",
    "    e[j] = 1.0\n",
    "    return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab45fd3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# bias examples \n",
    "set_biases([28,28,10],np.random)[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8e99c6",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# set weights examples \n",
    "set_weights(prepare_weightMatrix_dim([784,28,28,10]),np.random)[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce98d376",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "unit_vector(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d27432",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Writing the backpropagation algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465203fd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def backprop(net_input, y,biases,weights,cross_entropy):\n",
    "    a = [] # list to store all the activations, layer by layer\n",
    "    zs = [] # list to store all the z vectors, layer by layer\n",
    "\n",
    "    # Two empty vectors carrying the result \n",
    "    deriv_bias = [np.zeros_like(b) for b in biases]\n",
    "    deriv_weight = [np.zeros_like(w) for w in weights]\n",
    "\n",
    "    a.append(net_input)\n",
    "    # FORWARD RUN\n",
    "    # Iteration trough the layers, everything computed in vector form\n",
    "    for b, w in zip(biases, weights):\n",
    "        #compute each intermediate value z\n",
    "        z = np.dot(w, net_input)+b\n",
    "        #save it for later \n",
    "        zs.append(z)\n",
    "        #calculate and store each activation value \n",
    "        net_input = sigmoid(z)\n",
    "        a.append(net_input)\n",
    "        \n",
    "    # backward run trough the network \n",
    "    delta = quadcost_derivative(a[-1], y,zs[-1]) \n",
    "\n",
    "    deriv_bias[-1] = delta\n",
    "    deriv_weight[-1] = np.dot(delta, a[-2].transpose())\n",
    "    \n",
    "    #BACKWARD RUN \n",
    "    for l in range(-2, -len(sizes),-1):\n",
    "        z = zs[l]\n",
    "        sigma_prime = sigmoid_deriv(z)\n",
    "        # calculate current delta \n",
    "        # Hadamard product *\n",
    "        delta = np.dot(weights[l+1].transpose(), delta) * sigma_prime\n",
    "        # calculate each value for bias and weight, as in (BP3/4)\n",
    "        deriv_bias[l] = delta\n",
    "        deriv_weight[l] = np.dot(delta, a[l-1].transpose())\n",
    "    #return the two vectors \n",
    "    return (deriv_bias, deriv_weight)\n",
    "\n",
    "def quadcost_derivative(output_activations, y,z):\n",
    "        return ((output_activations-y) * sigmoid_deriv(z))\n",
    "\n",
    "def evaluation(test_data,biases,weights):\n",
    "        tmp = []\n",
    "        results = []\n",
    "        # we get the output for every test input\n",
    "        for x,y in test_data:\n",
    "            results.append((net_output(x,biases,weights),y))\n",
    "        for i in results:\n",
    "            # Argmax for finding the most likely result \n",
    "            tmp.append((np.argmax(i[0]),i[1]))\n",
    "        correct_classifications = 0\n",
    "        for (x,y) in tmp:\n",
    "            if(int(x)==int(y)):\n",
    "                correct_classifications = correct_classifications + 1\n",
    "        return correct_classifications\n",
    "    \n",
    "def net_output(net_input,biases,weights):\n",
    "    #Forward pass in the network \n",
    "    # get current output given input (evaluation purposes)    \n",
    "    for i in range(len(weights)):\n",
    "        current_z = np.dot(weights[i],net_input) + biases[i]\n",
    "        net_input = sigmoid(current_z)\n",
    "    return net_input\n",
    "\n",
    "def sigmoid(z):\n",
    "    try:\n",
    "        return 1.0/(1.0+np.exp(-z))\n",
    "    except:\n",
    "        # Overflow check \n",
    "        print(z)\n",
    "    \n",
    "def sigmoid_deriv(z):\n",
    "    return sigmoid(z)*(1-sigmoid(z))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4ec67b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now we define the main parameters of our network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f799ba",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0bffd88",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def prepare_data():\n",
    "    # # Reshape so that the input is a single array of dimension 784 \n",
    "    #(rather than 28x28)\n",
    "    test_imgs = np.reshape(x_test,[10000,784])\n",
    "    train_imgs = np.reshape(x_train,[60000,784])\n",
    "    \n",
    "    # Transform the data in a zip iterable object \n",
    "    training_inputs = [np.reshape(x,[784,1]) for x in train_imgs]\n",
    "    training_results = [unit_vector(y) for y in y_train]\n",
    "    training_data = zip(training_inputs, training_results)\n",
    "    test_inputs = [np.reshape(x, [784,1]) for x in test_imgs]\n",
    "    test_data = zip(test_inputs, y_test)\n",
    "\n",
    "    return (list(training_data), list(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69815771",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "training_data, test_data = prepare_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44965fac-9df9-48b2-a2a3-1dd30135d73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0144690",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "training_data[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9d8ce4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We define the main training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a94447",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(training_data, epochs, mini_batch_size, eta,test_data,weights,biases,rnd,plot=True,L2=True,cross_entropy=False):\n",
    "    # Start time, timestamps for tuning purposes \n",
    "    start = time.time()\n",
    "    train_evaluation = []\n",
    "    timestamps =  []\n",
    "    #Setup of logs for visualization purposes \n",
    "    biases_log = []\n",
    "    weights_log = []\n",
    "\n",
    "    n_test = len(test_data)\n",
    "    # cycle trough the requested number of epochs \n",
    "    for i in range(epochs):\n",
    "        # Shuffle the training data, so to achieve random batches\n",
    "        rnd.shuffle(training_data)\n",
    "        # split the data in batches\n",
    "        mini_batches = split_data(training_data, mini_batch_size)\n",
    "        print(\"mini batches number : \", len(mini_batches))\n",
    "\n",
    "        # SGD\n",
    "        # Update paramenters for each mini batch\n",
    "        for mini_batch in mini_batches:\n",
    "\n",
    "                # Prepare the lists for the partial derivatives of weights and biases\n",
    "                partial_deriv_biases = [np.zeros_like(b) for b in biases]\n",
    "                partial_deriv_weights = [np.zeros_like(w) for w in weights]\n",
    "\n",
    "                # for each training example run trough the network\n",
    "                for x, y in mini_batch:\n",
    "                    # we obtain from the network the two vectors \n",
    "                    # containing the partial derivatives with respect to weight and bias\n",
    "                    deriv_bias, deriv_weight = backprop(x, y,biases,weights,cross_entropy)\n",
    "                    #sum the values\n",
    "                    for l in range(len(partial_deriv_biases)):\n",
    "                        partial_deriv_biases[l] = partial_deriv_biases[l] + deriv_bias[l]\n",
    "                    for l in range(len(partial_deriv_weights)):\n",
    "                        partial_deriv_weights[l] = partial_deriv_weights[l] + deriv_weight[l]\n",
    "\n",
    "                # finally compute the updated values of weights and biases         \n",
    "                # Optional L2 regularization \n",
    "                for l in range(len(weights)):\n",
    "                        weights[l] = weights[l] - (eta/len(mini_batch))*partial_deriv_weights[l]\n",
    "\n",
    "                for l in range(len(biases)):\n",
    "                        biases[l] = biases[l] - (eta/len(mini_batch))*partial_deriv_biases[l]\n",
    "\n",
    "\n",
    "        # time evaluation\n",
    "        timestamps.append(time.time() - start)\n",
    "\n",
    "        #Get current net performance \n",
    "        v = evaluation(test_data,biases,weights)\n",
    "        print(\"{0} --> correct classifications: ({1} / {2}) \".format(\n",
    "            i, v, n_test))\n",
    "        # update the train evaluation list \n",
    "        train_evaluation.append(v)\n",
    "\n",
    "\n",
    "        # Following commands for plotting \n",
    "        if(plot==True):\n",
    "            level = 1 # choose which level of the network to plot \n",
    "            biases_log.append(biases[level])\n",
    "            weights_log.append(weights[level])\n",
    "\n",
    "    return train_evaluation, timestamps, biases_log, weights_log\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2f631f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Training the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ddb1d59",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#Set network architecture\n",
    "sizes = [784,28,28,10]\n",
    "#Set if unsaturated weights\n",
    "epochs = [3]\n",
    "mini_batches_len = [64]\n",
    "eta = [0.6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3689cd01",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "num_layers = len(sizes)\n",
    "# Prepare biases for each level except the first \n",
    "bias_matrix_dim = sizes[1:]\n",
    "# Prepare weights \n",
    "weight_matrix_dim = prepare_weightMatrix_dim(sizes)\n",
    "\n",
    "print(\"bias matrix dim\", bias_matrix_dim)\n",
    "print(\"weight matrix dim\",weight_matrix_dim)\n",
    "\n",
    "rnd = np.random.RandomState(121)\n",
    "\n",
    "biases = set_biases(bias_matrix_dim,copy.deepcopy(rnd))\n",
    "weights = set_weights(weight_matrix_dim,copy.deepcopy(rnd))\n",
    "\n",
    "train_evaluation = []\n",
    "timestamps = []\n",
    "train_evaluation2 = []\n",
    "timestamps2 = []\n",
    "\n",
    "# Load the data, as zip iterables \n",
    "training_data, test_data = prepare_data()\n",
    "\n",
    "# Set error so to be reactive to overflow \n",
    "#np.seterr(all='print')\n",
    "\n",
    "# Call the train method\n",
    "train_evaluation,timestamps, biases_log, weights_log = train(copy.deepcopy(training_data), epochs[0], mini_batches_len[0], eta[0],\n",
    "                                    copy.deepcopy(test_data),copy.deepcopy(weights),copy.deepcopy(biases),copy.deepcopy(rnd))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956c00f4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's visualize the changing of weights thorough the epochs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca95bd6",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "num_images = len(weights_log)\n",
    "image_data = weights_log\n",
    "\n",
    "# Set up the figure and axis\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(image_data[0], animated=True)\n",
    "\n",
    "def update(frame):\n",
    "    im.set_array(image_data[frame])\n",
    "    return im,\n",
    "\n",
    "ani = FuncAnimation(fig, update, frames=range(num_images), repeat=True)\n",
    "plt.close()\n",
    "# Display the animation\n",
    "HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821ae284",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "And the changing of biases "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eaf657c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "num_images = len(biases_log)\n",
    "image_data = biases_log\n",
    "\n",
    "# Set up the figure and axis\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(image_data[0], animated=True)\n",
    "\n",
    "def update(frame):\n",
    "    im.set_array(image_data[frame])\n",
    "    return im,\n",
    "\n",
    "ani = FuncAnimation(fig, update, frames=range(num_images), repeat=True)\n",
    "plt.close()\n",
    "# Display the animation\n",
    "HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7c7dd4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Plot a train evaluation\n",
    "fig2, ax = plt.subplots()\n",
    "ax.plot(timestamps,train_evaluation,color=\"red\",label=\"1- batchLen: {0}, eta: {1}, epochs: {2}\".format(mini_batches_len[0],eta[0],epochs[0]))\n",
    "ax.set(xlabel='time (s)', ylabel='score',\n",
    "       title='train evaluation')\n",
    "ax.grid()\n",
    "ax.legend(shadow=True, fontsize=\"large\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d82e28",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Tf/Keras\n",
    "A powerful framework for neural network "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3894caf2",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "TensorFlow is an open-source platform designed for high performance numerical computation, developed by the google brain team. It facilitates the creation of large-scale neural networks with numerous layers, thanks to its automatic differentiation capability and strong support for deep learning models.\n",
    "\n",
    "At the heart of TensorFlow's functionality is the ability to execute intensive mathematical operations on a large scale and with high efficiency. This is particularly beneficial in training complex neural networks, where such operations are widespread. TensorFlow excels in scalability and flexibility, allowing it to run on multiple CPUs and GPUs, as well as on mobile devices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9baa189e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Keras, integrated into TensorFlow as tf.keras, provides a more accessible interface to the TensorFlow library by abstracting away much of the complexity. Keras is user-friendly and modular, making it simple for beginners to build and experiment with neural networks. Its API is designed with human beings in mind, not just machines, offering a balance between ease of use and flexibility."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8cd1147",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In practive, __Keras provides a user-friendly framework for building neural networks, while TensorFlow offers granular control for specialized tasks within the keras Keras implementation__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4a504a-0e30-4fef-b9be-8258eeecfeba",
   "metadata": {},
   "source": [
    "### Keras 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af0a951-30e2-427e-abde-39efffccf4b3",
   "metadata": {},
   "source": [
    "The keras 3 core, released in december 2023, effectively utilize three different backends JAX, TF and pytorch. Very powerful indeed!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3699df79",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Keras Sequential \n",
    "The Keras Sequential API is a straightforward yet powerful tool for creating neural network models in Python. It allows developers to construct models layer-by-layer in a step-by-step fashion. The Sequential model is a linear stack of layers, making it ideal for building simple, yet deep, neural networks without the complexity of graph-like architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed154be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ[\"KERAS-BACKEND\"] = \"torch\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Dropout\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564a1c97",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Flatten(input_shape=(28 *28,)),\n",
    "    Dense(784, activation = \"swish\"),\n",
    "    Dropout(0.3),\n",
    "    Dense(100, activation = \"swish\"),\n",
    "    Dense(10, activation = \"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce70108",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='mse',\n",
    "              metrics=['accuracy'])\n",
    "# kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01)\n",
    "# How is L2 regularization added to the network?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838d61f1",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Load and preprocess the data\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "train_images = train_images.reshape((60000, 28 * 28))\n",
    "train_images = train_images.astype('float32') / 255\n",
    "test_images = test_images.reshape((10000, 28 * 28))\n",
    "test_images = test_images.astype('float32') / 255\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60591651",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = model.fit(train_images, train_labels, epochs=5, batch_size=128)\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "print(f'Test accuracy: {test_acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a802143-cd11-4aa1-8310-7e55700f2322",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = history.history[\"accuracy\"]\n",
    "loss = history.history[\"loss\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a50987-8cd1-4da5-884a-6f521f770124",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea747cc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How to choose an activation function?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aea00bb",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The choice is often quite straight forward:\n",
    " - If I have a classification problem for the last layer the standard is the softmax, returning a probability distribution of the outcomes (the sum of all the probability for all classes is 1)\n",
    " - In the case of a binary classification problem Sigmoid is the most common function, similarly to softmax\n",
    " - for deep layers, ReLu is commonly the standard, mainly because of it's effectiveness in avoiding the vanishing gradient problem. Other common issues of ReLu (dying neurons) are solved via the use of LeakyRelu, Swish-\n",
    " - tanh mean is 0, which can help stabilize the network and avoid axploding gradient, such as in recurrent neural networks. \n",
    "- The Exponential Linear Unit (ELU) activation function, on the other hand, has a small slope for negative values, which helps reduce the vanishing gradient problem, allowing the model to learn even when the neurons are in the negative saturation region. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64bcb86c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Activation functions\n",
    "sigmoid = lambda x: 1 / (1 + np.exp(-x))\n",
    "tanh = lambda x: np.tanh(x)\n",
    "relu = lambda x: np.maximum(0, x)\n",
    "leaky_relu = lambda x, alpha=0.01: np.where(x > 0, x, x * alpha)\n",
    "swish = lambda x: x / (1 + np.exp(-x))\n",
    "elu = lambda x, alpha=1.0: np.where(x > 0, x, alpha * (np.exp(x) - 1))\n",
    "\n",
    "# Range of values\n",
    "x = np.linspace(-3, 3, 200)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "for i, (func, title) in enumerate(zip([sigmoid, tanh, relu, leaky_relu, swish, elu], \n",
    "                                      [\"Sigmoid\", \"Tanh\", \"ReLU\", \"Leaky ReLU\", \"Swish\", \"ELU\"])):\n",
    "    plt.subplot(2, 3, i+1)\n",
    "    plt.plot(x, func(x))\n",
    "    plt.title(title)\n",
    "    plt.grid()\n",
    "    plt.ylim([-1.5, 3])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a871f8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How to choose an optimizer?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e615b9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We will focus just on the default optimizers provided by Keras: \n",
    " - SGD, simplest and most efficient optimizer. In Keras also support momentum based optimization. Good for large datasets and simple optimization problems. \n",
    " - Adagrad - Adjusts the learning rate for each parameter based on the historical sum of squares of the gradients, which means it gives smaller updates for frequently occurring features, can be good for sparse data. \n",
    " - RMSProp, utilizes the moving average of squared gradients to normalize the gradient, this means it adapts the learning rate for each weight. Can be seen as an updated version of Adagrad. \n",
    " - Adam (Adaptive Momentum Estimation), Combines the benefits of AdaGrad and RMSprop, adjusting the learning rate for each weight based on the first (mean) and second (uncentered variance) moments of the gradients. Very popular due to its robust performance across a wide variety of models and problems. Itâ€™s often the first choice in many deep learning applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1badaacb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Usually, Adam is a good starting choice for most deep learning applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13099693",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How to choose a loss?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de35dd8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The loss function is a one of the most problem-specific part of a neural network. It is often the case that a custom loss function is needed. Generally speaking, if the problem is a regression (continous output) a MAE or MSE is commonly used. \n",
    "\n",
    "If the problem is a classification (binary or multi class) a variation of categorical cross-entropy is often utilizied. \n",
    "\n",
    "In conclusion, for what concerns losses, optimizers and activations consider that some practical results are often necessary other than the more theoretical aspects. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8070f196",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Keras Functional API\n",
    "The Keras Functional API is a way to create models that are more flexible than the Sequential API, which allows you to create models that cannot be defined with a linear stack of layers. With the Functional API, you can manipulate the inputs and outputs, and build models that have multiple inputs and outputs, shared layers, and even non-sequential data flows like residual connections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4602ebe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Flatten, Add, BatchNormalization, Dropout\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# Load and preprocess the data (same as above)\n",
    "\n",
    "# Build the model using the Functional API\n",
    "inputs = Input(shape=(28 * 28,))\n",
    "x = Flatten()(inputs)\n",
    "x = Dense(784, activation=\"relu\")(x)\n",
    "outputs = Dense(10, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_images, train_labels, epochs=5, batch_size=128)\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "print(f'Test accuracy: {test_acc}')\n",
    "#x2 = Dense(128, activation='relu')(x)\n",
    "#x = Add()([x1,x2])\n",
    "#shared = Dense(784, activation='relu')\n",
    "#x = shared(x)\n",
    "#x = shared(x)\n",
    "#x = BatchNormalization()(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e28cc4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Can you guess what Dropout and BatchNormalization do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e81924e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e5f928",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Other model examples utilizing the Keras Functional API, multiple inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8cae0a2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Just a model example, not meant to be executed\n",
    "from keras.layers import Input, Embedding, LSTM, Dense, concatenate\n",
    "from keras.models import Model\n",
    "\n",
    "# First input: Categorical data that we will embed\n",
    "categorical_input = Input(shape=(10,), dtype='int32', name='categorical_input')\n",
    "x = Embedding(output_dim=512, input_dim=10000, input_length=10)(categorical_input)\n",
    "lstm_out = LSTM(32)(x)\n",
    "\n",
    "# Second input: Numerical data\n",
    "numerical_input = Input(shape=(5,), name='numerical_input')\n",
    "n = Dense(32, activation='relu')(numerical_input)\n",
    "\n",
    "# Merge all available features into a single large vector via concatenation\n",
    "x = concatenate([lstm_out, n])\n",
    "\n",
    "# We stack a deep densely-connected network on top\n",
    "x = Dense(64, activation='relu')(x)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "\n",
    "# And finally add the main logistic regression layer\n",
    "main_output = Dense(1, activation='sigmoid', name='main_output')(x)\n",
    "\n",
    "model = Model(inputs=[categorical_input, numerical_input], outputs=[main_output])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c182c592",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Model API with custom dense layer and Custom gradient loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad85f8a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "from keras.layers import Layer, Input, Dense, Flatten, BatchNormalization\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# Custom Layer Example\n",
    "class CustomDense(Layer):\n",
    "    def __init__(self, units, activation=None):\n",
    "        super(CustomDense, self).__init__()\n",
    "        self.units = units\n",
    "        self.activation = keras.activations.get(activation)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.w = self.add_weight(shape=(input_shape[-1], self.units),\n",
    "                                 initializer='random_normal',\n",
    "                                 trainable=True)\n",
    "        self.b = self.add_weight(shape=(self.units,),\n",
    "                                 initializer='zeros',\n",
    "                                 trainable=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.activation(tf.matmul(inputs, self.w) + self.b)\n",
    "\n",
    "# Load and preprocess the data (same as above)\n",
    "\n",
    "# Build the model with a custom layer\n",
    "inputs = Input(shape=(28 * 28,))\n",
    "x = Flatten()(inputs)\n",
    "x = CustomDense(512, activation='relu')(x)\n",
    "x = CustomDense(128, activation='relu')(x)\n",
    "outputs = CustomDense(10, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2425f9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A Simple Gradient Tape Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6241edee",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Custom training loop\n",
    "optimizer = tf.keras.optimizers.RMSprop()\n",
    "loss_fn = tf.keras.losses.CategoricalCrossentropy()\n",
    "\n",
    "# Prepare the training dataset\n",
    "batch_size = 64\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
    "\n",
    "for epoch in range(5):\n",
    "    print(epoch)\n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = model(x_batch_train, training=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9616bcb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A more advanced loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d882bb6e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Custom training loop\n",
    "optimizer = tf.keras.optimizers.RMSprop()\n",
    "loss_fn = tf.keras.losses.CategoricalCrossentropy()\n",
    "train_acc_metric = tf.keras.metrics.CategoricalAccuracy()\n",
    "val_acc_metric = tf.keras.metrics.CategoricalAccuracy()\n",
    "\n",
    "# Prepare the training dataset\n",
    "batch_size = 64\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
    "\n",
    "# Prepare the validation dataset\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((test_images, test_labels))\n",
    "val_dataset = val_dataset.batch(batch_size)\n",
    "\n",
    "# Training and validation loop\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    print(\"\\nStart of epoch %d\" % (epoch,))\n",
    "\n",
    "    # Reset the metrics at the start of the next epoch\n",
    "    train_acc_metric.reset_states()\n",
    "\n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = model(x_batch_train, training=True)  # Logits for this minibatch\n",
    "            # Compute the loss value for this minibatch\n",
    "            loss_value = loss_fn(y_batch_train, logits)\n",
    "\n",
    "        # Use the gradient tape to automatically retrieve\n",
    "        # the gradients of the trainable variables with respect to the loss.\n",
    "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "        # Run one step of gradient descent by updating\n",
    "        # the value of the variables to minimize the loss.\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "\n",
    "        # Update training metric.\n",
    "        train_acc_metric.update_state(y_batch_train, logits)\n",
    "\n",
    "        # Log every 200 batches.\n",
    "        if step % 200 == 0:\n",
    "            print(\"Training loss (for one batch) at step %d: %.4f\" % (step, loss_value.numpy()))\n",
    "            print(\"Seen so far: %d samples\" % ((step + 1) * batch_size))\n",
    "\n",
    "    # Display metrics at the end of each epoch.\n",
    "    train_accuracy = train_acc_metric.result()\n",
    "    print(\"Training acc over epoch: %.4f\" % (float(train_accuracy),))\n",
    "\n",
    "    # Reset the validation metrics\n",
    "    val_acc_metric.reset_states()\n",
    "\n",
    "    # Perform validation at the end of each epoch\n",
    "    for x_batch_val, y_batch_val in val_dataset:\n",
    "        val_logits = model(x_batch_val, training=False)\n",
    "        # Update val metrics\n",
    "        val_acc_metric.update_state(y_batch_val, val_logits)\n",
    "    val_accuracy = val_acc_metric.result()\n",
    "    print(\"Validation acc: %.4f\" % (float(val_accuracy),))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a9ad79",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Some notes on Dropout and Batch Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5f93f9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "__Batch normalization__ is important in neural architectures because it helps in stabilizing and accelerating the training process by normalizing the inputs of each layer. It does this by adjusting and scaling the activations, which allows each layer of a network to learn more independently of other layers. \n",
    "\n",
    "This normalization helps to combat the internal covariate shift problem, where the distribution of input data keeps changing during training, making it difficult for the network to converge. Additionally, batch normalization often allows for the use of higher learning rates and reduces the sensitivity to weight initialization, both of which can lead to faster convergence. It can also act as a form of regularization, potentially reducing the need for other regularization techniques like dropout.\n",
    "\n",
    "__Dropout__ on the other hand is a regularization technique used in neural networks to prevent overfitting. It works by randomly dropping a subset of neurons during each training iteration, which helps to break up co-adaptations where neurons rely too heavily on the presence of particular other neurons. This encourages the network to develop more robust features that are useful in conjunction with many different random subsets of the other neurons. Dropout can significantly improve the performance of neural networks on test data by promoting the creation of a more generalized model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d71a70",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can now start exploring the default library of keras examples, full of great starting point for neural projects. \n",
    "https://keras.io/examples/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28890502",
   "metadata": {},
   "source": [
    "See you next time!"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
